# Redis Database Structure Guide

**Target audience:** Engineers ramping up on the project. This guide assumes general familiarity with Redis but no prior context on this system.

---

## Overview

Redis serves as the **primary caching layer** for the movie search platform. It is self-hosted as a Docker container on the same EC2 t3.large instance as the rest of the stack (Postgres, Qdrant, API server, Nginx). It is not exposed to the internet — all access happens over Docker's internal network.

Redis is allocated approximately **200–500 MB of RAM** in the overall EC2 budget. This is feasible because the four namespaces it manages store compact, flat data: serialized JSON blobs, packed binary float arrays, and a small set of integer IDs.

Redis in this system is **never a source of truth**. It is always a cache of data that either lives elsewhere (TMDB, OpenAI, Postgres) or can be regenerated deterministically from a known input. Every key either has an explicit TTL or is managed via atomic overwrite by a background job. There is no cache invalidation by dependency — stale keys expire and are repopulated on the next request.

---

## Environment / Namespace Prefixing

**All keys must be prefixed with the deployment environment.** This prevents staging and production from colliding if they ever share a Redis instance, and prevents an accidental misconfiguration from corrupting live data.

The prefix convention is `{env}:` prepended to every key:

```
prod:emb:{model}:{hash}
prod:qu:v{N}:{hash}
prod:trending:current
prod:tmdb:movie:{id}

staging:emb:{model}:{hash}
staging:qu:v{N}:{hash}
...
```

Set the environment prefix via a config variable at startup. Every Redis read and write in the application must use the prefixed key. Never hardcode the environment into application code.

---

## The Four Namespaces

| Key Pattern | Redis Type | Contents | TTL |
|---|---|---|---|
| `{env}:emb:{model}:{hash}` | String | Packed binary float32 array (embedding vector) | 7 days |
| `{env}:qu:v{N}:{hash}` | String | Serialized JSON blob (full query understanding output) | 1 day |
| `{env}:trending:current` | Set | Integer movie IDs currently trending | No TTL — atomic overwrite only |
| `{env}:tmdb:movie:{id}` | String | TMDB detail JSON blob for a single movie | 1 day |

Each namespace is covered in full detail below. The `{env}:` prefix is omitted from key examples throughout for readability, but must always be present in code.

---

## 1. Embedding Cache — `emb:{model}:{hash}`

### What it stores

A **packed binary float32 array** representing a single 1536-dimensional text embedding produced by OpenAI's embedding API. The vector is stored as raw bytes using `struct.pack` — not as a JSON array of floats. This matters for memory: a JSON-encoded 1536-float array is roughly 10–15 KB; the packed binary form is exactly **6,144 bytes** (1536 × 4 bytes). Always store and retrieve using binary serialization.

```python
# Writing
import struct
packed = struct.pack(f'{len(vector)}f', *vector)
redis.set(key, packed, ex=ttl)

# Reading
raw = redis.get(key)
vector = list(struct.unpack(f'{len(raw)//4}f', raw))
```

### Key construction

```
emb:{model}:{hash(normalized_text)}
```

- `{model}` — the full OpenAI embedding model name (e.g., `text-embedding-3-small`). Embedding output is model-specific; including the model name prevents different models from colliding under the same key. Changing models requires no invalidation — old keys expire, new keys populate under the new model name.
- `{hash}` — SHA256 of the **normalized** input text. Normalization for embeddings: trim leading/trailing whitespace, collapse internal whitespace to a single space. **Do not lowercase** — embedding models are case-sensitive and preserving case gives more accurate results. Use the first 16 hex characters of the SHA256 digest (collision risk is negligible at this data volume).

> **Note on dimensions:** OpenAI's `text-embedding-3` models support a `dimensions` parameter to reduce output size. If you ever use this parameter, include the dimension count in the model component of the key (e.g., `text-embedding-3-small-512`) to prevent collisions with full-dimension vectors stored under the same model name.

### What gets embedded (and cached here)

For every search request, embeddings are needed for:

1. The **original user query** — always embedded; used for the Anchor vector channel and as a fallback for all channels with non-zero relevance
2. Each **per-channel subquery** — up to 7 subqueries generated by the query understanding DAG, one per vector collection: `plot_events`, `plot_analysis`, `viewer_experience`, `watch_context`, `narrative_techniques`, `production`, `reception`

Only subqueries with a non-null `relevant_subquery_text` in the QU output are embedded. Subqueries with a null value are skipped entirely.

### How it's used in a request

During Step 2 of the request flow, the API server iterates over every text that needs embedding. These lookups run concurrently:

1. Normalize the text (trim + whitespace collapse, no lowercasing)
2. Compute `SHA256(normalized_text)[:16]`
3. Check Redis — **hit → deserialize and return the packed vector immediately**
4. **Miss → call OpenAI Embeddings API → pack binary → store in Redis with TTL → continue**

### TTL: 7 days

Embeddings are pure mathematical outputs: for a fixed model and input text, the output is always identical. Seven days means returning users with similar queries benefit from the cache across multiple sessions. At 6 KB per vector, even tens of thousands of cached embeddings stay well within budget.

---

## 2. Query Understanding Cache — `qu:v{N}:{hash}`

### What it stores

A serialized JSON blob representing the **complete structured output of the query understanding DAG**. This is the highest-impact cache in the system. A cache hit skips approximately 24 parallel LLM calls, which is the dominant latency source in a search request. Measure actual DAG latency in your environment — it will vary based on model, prompt length, and concurrency.

### Key construction

```
qu:v{N}:{hash(query_text)}
```

- `v{N}` — an integer version tag (e.g., `v1`, `v2`). See **Versioning Rules** below.
- `{hash}` — SHA256 of the **minimally normalized** query text: trim whitespace and collapse internal runs of whitespace to one space. **Do not lowercase.** This is intentional and important — see the note below.

> **Why not lowercase the QU cache key?**
> Lowercasing is correct for *lexical entity matching* (handled separately in Postgres), but wrong for query understanding cache keys. It collapses semantically distinct queries into the same cache slot. Concrete examples: the film **"It"** and the pronoun "it"; the film **"Her"** and the pronoun "her"; the film **"Us"** and the word "us". Serving a cached DAG result for "it" (the pronoun query) when the user typed "It" (the film) would return a completely wrong query decomposition with no error. The normalization for this cache key must be conservative — whitespace collapsing only.

### Versioning rules

The `v{N}` tag must be incremented whenever **any** of the following change:

- Any LLM system prompt used in the DAG
- The LLM model name (e.g., switching from `gpt-4o` to `gpt-4o-mini`)
- The `QueryUnderstandingResponse` schema (adding, removing, or renaming fields)
- LLM temperature, top_p, or any sampling parameter
- The query text normalization function itself

When `N` is bumped, old `qu:v{N-1}:*` keys expire harmlessly within 1 day — no manual cleanup needed. Store `N` as a named constant in a shared config, not as a magic number in application code. Treat a version bump as a required step in any deployment that touches the DAG.

### Schema of the stored blob

The blob is the serialized `QueryUnderstandingResponse` object. Its full structure:

```json
{
  "channel_weights": {
    "lexical_relevance":  "not_relevant | small | medium | large",
    "metadata_relevance": "not_relevant | small | medium | large",
    "vector_relevance":   "not_relevant | small | medium | large"
  },
  "lexical_entities": {
    "entity_candidates": [
      {
        "candidate_entity_phrase": "string",
        "most_likely_category": "movie_title | person | character | studio | franchise",
        "exclude_from_results": false,
        "corrected_and_normalized_entity": "string"
      }
    ]
  },
  "metadata_preferences": {
    "release_date_preference": {
      "result": {
        "first_date": "YYYY-MM-DD",
        "match_operation": "exact | before | after | between",
        "second_date": "YYYY-MM-DD | null"
      }
    },
    "duration_preference": { "result": null },
    "genres_preference": {
      "result": { "should_include": ["string"], "should_exclude": ["string"] }
    },
    "audio_languages_preference": { "result": null },
    "watch_providers_preference": { "result": null },
    "maturity_rating_preference": { "result": null },
    "popular_trending_preference": {
      "prefers_trending_movies": false,
      "prefers_popular_movies": false
    },
    "reception_preference": {
      "reception_type": "no_preference | critically_acclaimed | poorly_received"
    }
  },
  "vector_subqueries": {
    "plot_events_data":          { "justification": "string", "relevant_subquery_text": "string | null" },
    "plot_analysis_data":        { "justification": "string", "relevant_subquery_text": "string | null" },
    "viewer_experience_data":    { "justification": "string", "relevant_subquery_text": "string | null" },
    "watch_context_data":        { "justification": "string", "relevant_subquery_text": "string | null" },
    "narrative_techniques_data": { "justification": "string", "relevant_subquery_text": "string | null" },
    "production_data":           { "justification": "string", "relevant_subquery_text": "string | null" },
    "reception_data":            { "justification": "string", "relevant_subquery_text": "string | null" }
  },
  "vector_weights": {
    "plot_events_data":          { "relevance": "not_relevant | small | medium | large", "justification": "string" },
    "plot_analysis_data":        { "relevance": "...", "justification": "string" },
    "viewer_experience_data":    { "relevance": "...", "justification": "string" },
    "watch_context_data":        { "relevance": "...", "justification": "string" },
    "narrative_techniques_data": { "relevance": "...", "justification": "string" },
    "production_data":           { "relevance": "...", "justification": "string" },
    "reception_data":            { "relevance": "...", "justification": "string" }
  }
}
```

### What each field drives downstream

| Field | Consumed by |
|---|---|
| `channel_weights` | Weights the relative contribution of lexical, metadata, and vector scores in the final reranking formula |
| `lexical_entities` | Drives lexical search queries against Postgres — title token lookups, people/character/studio phrase lookups |
| `metadata_preferences` | Drives Qdrant hard filter construction; per-candidate metadata scoring during reranking |
| `popular_trending_preference.prefers_trending_movies` | If `true`, triggers a Redis `SMEMBERS` fetch of `trending:current` and applies a trending bonus during reranking |
| `vector_subqueries` | Each non-null `relevant_subquery_text` is embedded (Step 2) and used to query its corresponding Qdrant named vector collection |
| `vector_weights` | Determines which Qdrant collections are queried and how their scores are weighted during reranking |

> **Field name callout:** The trending bonus is gated on `popular_trending_preference.prefers_trending_movies`, a nested boolean. There is no top-level `prefers_trending_now` field in this schema. Using the wrong field name produces a silent bug where trending boost never applies — double-check this in any reranking code.

### Critical: always store atomically as a single key

The blob must be written as **one atomic key** representing the entire DAG output. Never cache partial DAG results (e.g., one key per stage). A partial write followed by a request reading those partial results can produce a mix of outputs from different executions, causing silent consistency bugs. One key, one `SET`, one TTL, written only after the full DAG completes successfully.

### Why this is safe to cache

The DAG output is a deterministic function of the query text, given a fixed model, prompts, temperature, and schema. The system never bakes session-specific state into this blob — `shown_movie_counts`, user-selected UI filters, and the trending set are all applied downstream, after the cache is read.

### TTL: 1 day

One day balances freshness with hit rate. The most common queries (genre searches, actor names, broad themes) are highly cacheable and benefit from this TTL across the typical user session window.

---

## 3. Trending Set — `trending:current`

### What it stores

A Redis **Set** of integer `movie_id` values for movies currently considered trending. Membership in this set is used during reranking to apply a score bonus when the user's query specifies trending preference (`popular_trending_preference.prefers_trending_movies: true` in the QU output).

### Key

```
trending:current
```

One fixed key. There is only ever one active trending set.

### How it's populated — atomic RENAME pattern

The daily cron job (runs at midnight) must refresh this key **atomically** to avoid any window where the key is empty or partially populated. The correct pattern:

```
1. Fetch current trending movie IDs from upstream source
2. Write the full new set to a staging key:
     DEL trending:next
     SADD trending:next <id1> <id2> ... <idN>
3. Atomically swap into place:
     RENAME trending:next trending:current
4. Optionally persist the set to Postgres for auditing
```

`RENAME` is atomic in Redis — it replaces `trending:current` with `trending:next` in a single operation with no gap between deletion and insertion. **Do not use `DEL trending:current` + `SADD trending:current ...` in sequence** — that creates a window where the key is absent, during which concurrent requests silently skip the trending bonus and log spurious warnings.

### TTL: none — rely on atomic overwrite

`trending:current` carries **no TTL**. The key lives indefinitely and is replaced atomically by each successful daily run. This is a deliberate decision over a 1-day TTL:

- With a matching TTL (1 day), a delayed or failed cron job causes the key to expire before the replacement arrives, creating a silent gap in trending data.
- With no TTL, a failed job leaves a stale-but-valid trending set in place until the next successful run. A stale trending set is a much better failure mode than a missing one.

> **Eviction safety:** Because `trending:current` has no TTL, it is immune to eviction under the `volatile-lru` memory policy (see Memory Configuration below). This is intentional.

### How it's used in a request

During Step 5 of the request flow, the API server fetches `trending:current` **once per request** and loads it into an in-memory set. All membership checks during reranking (Step 6) are O(1) in-process lookups — Redis is never queried per-candidate.

```python
# Pseudocode — Step 5 of request flow
trending_ids = redis.smembers("{env}:trending:current")  # one call per request
trending_set = set(int(id) for id in trending_ids)       # in-memory for reranking

# Step 6 — reranking loop
for candidate in candidates:
    is_trending = candidate.movie_id in trending_set     # O(1), no Redis I/O
```

If the key is missing (cron has never run, or the key was manually deleted), treat the set as empty and log a `WARNING` — do not fail the request. Missing trending data is a graceful degradation.

### Why a Redis Set

A native Redis Set enables the `RENAME`-based atomic overwrite pattern and provides a single-call `SMEMBERS` retrieval. The set stays small in practice — trending lists are typically tens to a few hundred movies — so `SMEMBERS` is a negligible network payload.

---

## 4. TMDB Detail Cache — `tmdb:movie:{id}`

### What it stores

A JSON blob containing movie detail data from The Movie Database (TMDB) API. This typically includes: title, overview, tagline, runtime, release date, genres, cast, crew, production companies, budget, revenue, spoken languages, backdrop/poster images, and any appended sub-resources (videos, etc.).

**Shape the TMDB response server-side before caching it.** Do not cache the raw TMDB payload verbatim. TMDB endpoints using `append_to_response` expansions (credits, videos, images) can return very large responses depending on which expansions you use. Measure the serialized size of a representative sample of movies from your catalog and confirm it fits within your Redis memory budget. Do not assume a fixed KB range per movie — it varies significantly by how much cast/crew data a film has.

Cache the shaped response — the exact JSON your `GET /movie/{id}` endpoint returns to clients, not the raw upstream blob.

### Key construction

```
tmdb:movie:{movie_id}
```

`{movie_id}` is the internal integer ID used throughout the system — the same ID that serves as the Qdrant point ID and the Postgres primary key. The server maps to `tmdb_id` before calling TMDB, but the cache key uses the internal ID.

> **Request variant note:** This key assumes a single canonical TMDB request shape — one fixed set of `append_to_response` parameters, one language, one region. If you ever need to vary these per request (e.g., localized responses for different markets), the key must include those parameters (e.g., `tmdb:movie:{id}:en-US`) to prevent one variant from being served to a client expecting another.

### How it's used

The TMDB cache is accessed **only on detail page loads**, never during search. The flow for `GET /movie/{id}`:

1. Check `tmdb:movie:{id}` in Redis
   - **Hit** → return cached JSON immediately
   - **Miss** → call TMDB API → shape response → store in Redis with 1-day TTL → return to client

This keeps TMDB API credentials server-side and eliminates redundant external calls for popular movies that many users open.

### TTL: 1 day

TMDB data for established films changes infrequently. Note that **watch provider availability** is managed separately via the weekly watch offers refresh job on Postgres and Qdrant — the TMDB cache is for detail page content only and is not used in search filtering or reranking.

---

## Operational Configuration

### Memory policy: `volatile-lru`

Set `maxmemory-policy volatile-lru` in your Redis configuration. This policy **only evicts keys that have a TTL set**, leaving keys without a TTL permanently safe from eviction under memory pressure.

Under `volatile-lru`:
- `emb:*`, `qu:*`, and `tmdb:*` keys all carry TTLs and are eligible for LRU eviction — the least-recently-used entries in these namespaces are dropped first when memory is tight.
- `trending:current` has no TTL and is **never eligible for eviction**. This is correct — it's the only key in Redis that cannot be regenerated by the next incoming request.

Do **not** use `allkeys-lru`. That policy would allow Redis to silently evict `trending:current` under memory pressure, causing intermittent trending degradation that is difficult to diagnose because it produces no errors — just subtly worse ranking.

Set `maxmemory` to a conservative ceiling (e.g., 450 MB) to leave headroom for the other services on the t3.large.

### Serialization format for embeddings

Always use packed binary float32 for embedding vectors, not JSON arrays. The difference is significant:

| Format | Size per 1536-dim vector |
|---|---|
| JSON array of floats | ~10–15 KB |
| Packed binary float32 (`struct.pack`) | 6,144 bytes |

The embedding cache is the largest Redis namespace by volume at steady state. Using JSON accidentally can exhaust the memory budget 2–3× faster than expected with no obvious warning until `volatile-lru` starts aggressively evicting useful entries.

### Stampede protection

Even at low QPS, cold starts after a deploy or a Redis restart can cause a burst of simultaneous cache misses that all race to call OpenAI or run the LLM DAG in parallel. For the QU cache specifically — where a miss triggers ~24 LLM calls — this can produce a spike in cost and latency that's disproportionate to the traffic.

Implement a lightweight coalescing lock: before running the DAG (or an embedding call), attempt `SET lock:{key} 1 NX EX 10`. If the lock is acquired, perform the work and populate the cache. If the lock is not acquired, wait briefly and retry the cache read. This ensures only one request performs the expensive operation per unique query during a cold burst.

### Cache miss behavior summary

| Namespace | On miss | Request fails? |
|---|---|---|
| `emb:*` | Call OpenAI Embeddings API, cache result | No — higher latency only |
| `qu:*` | Run full LLM DAG (~24 parallel calls), cache result | No — unless DAG itself fails after retries |
| `trending:current` | Treat as empty set, log `WARNING` | No — trending bonus silently skipped |
| `tmdb:*` | Call TMDB API, shape and cache result | No — higher latency only |

### Monitoring

Add the following to your per-request structured logs:

- QU cache: hit or miss — this is the single most important latency signal in the system
- Embedding cache: hit/miss count per request
- `trending:current` missing: log at `WARNING` level so it's visible in dashboards

Run `redis-cli INFO memory` to monitor `used_memory_rss` over time. If it grows unexpectedly, investigate by namespace: embeddings are the most likely culprit at sustained load; TMDB blobs are the wildcard if your shaped response size is larger than anticipated.

### Persistence

Redis is configured as a **pure cache** — RDB snapshots and AOF persistence are not needed. A Redis restart cold-starts cleanly and warms up through normal traffic. Nothing stored in Redis is a source of truth: embeddings and QU blobs regenerate on demand, TMDB responses can be re-fetched, and the trending set is replaced by the next cron run.