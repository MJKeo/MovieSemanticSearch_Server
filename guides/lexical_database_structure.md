# Lexical Database Structure Guide (Postgres)

This is a technical specification for implementing the **lexical search database** in Postgres. It defines **exact tables**, their **schemas**, and **why each exists**, aligned with our current lexical search logic:

* 4 lexical buckets: **movie_title tokens**, **people phrases**, **character phrases**, **studio phrases**
* metadata hard filters are **AND** (pushdown eligible set)
* lexical matching is **OR** across buckets (candidate if it matches at least one)
* title tokens use **fuzzy matching (edit distance ≤ 1)** + **max_df (≤ 10,000)**
* title scoring uses the existing guide policy (β=2, threshold=0.15) and requires `title_token_count` from metadata
* dictionary is global string→id mapping for all normalized strings

---

## 0) Postgres prerequisites

### Extensions

Required for fuzzy title-token lookup:

```sql
CREATE EXTENSION IF NOT EXISTS pg_trgm;
CREATE EXTENSION IF NOT EXISTS fuzzystrmatch;
```

### Schema namespace

All objects live under a dedicated schema:

```sql
CREATE SCHEMA IF NOT EXISTS lex;
```

---

## 1) `lex.lexical_dictionary`

### Purpose

A **global, stable mapping** from a normalized string to an integer ID (`string_id`). This enables:

* compact storage in posting tables (`BIGINT` instead of `TEXT`)
* fast equality lookup for phrase buckets (people/characters/studios)
* fuzzy lookup for title tokens (via trigram + levenshtein)

### Schema

```sql
CREATE TABLE lex.lexical_dictionary (
  string_id   BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  norm_str    TEXT NOT NULL UNIQUE,
  touched_at  TIMESTAMP NOT NULL DEFAULT now(),
  created_at  TIMESTAMP NOT NULL DEFAULT now()
);

-- exact lookup
CREATE INDEX idx_lex_dict_norm_str
  ON lex.lexical_dictionary (norm_str);

-- fuzzy shortlist (only used for title tokens)
CREATE INDEX idx_lex_dict_norm_str_trgm
  ON lex.lexical_dictionary USING GIN (norm_str gin_trgm_ops);
```

### Notes

* `norm_str` must be produced by the **same normalizer** at ingest and query time.
* `touched_at` is useful for debugging, cache-warming strategies, or future maintenance; it is not required by query logic.

---

## 2) `lex.movies`

### Purpose

Stores **only the metadata required for hard filters** plus `title_token_count` needed for title scoring.

Lexical postings are **not** stored as arrays here (postings-only design).

### Schema

```sql
CREATE TABLE lex.movies (
  movie_id          BIGINT PRIMARY KEY,     -- tmdb_id

  -- Hard filter attributes
  release_ts        BIGINT,                 -- Unix seconds (midnight UTC)
  runtime_minutes   INT,
  maturity_rank     SMALLINT,               -- ordinal mapping in server code

  genre_ids         INT[] NOT NULL DEFAULT '{}',

  -- watch availability encoded pairs (provider_id, watch_method_id)
  watch_offer_keys  INT[] NOT NULL DEFAULT '{}',

  -- Title scoring input
  title_token_count INT NOT NULL DEFAULT 0,

  updated_at        TIMESTAMP NOT NULL DEFAULT now(),
  created_at        TIMESTAMP NOT NULL DEFAULT now()
);

-- Range filters
CREATE INDEX idx_movies_release_ts      ON lex.movies (release_ts);
CREATE INDEX idx_movies_runtime_minutes ON lex.movies (runtime_minutes);
CREATE INDEX idx_movies_maturity_rank   ON lex.movies (maturity_rank);

-- Overlap filters (any-of)
CREATE INDEX idx_movies_genre_ids
  ON lex.movies USING GIN (genre_ids);

CREATE INDEX idx_movies_watch_offer_keys
  ON lex.movies USING GIN (watch_offer_keys);
```

### Why `watch_offer_keys` exists (instead of separate arrays)

The UI can express **paired constraints** like “Amazon **rent**” (rent specifically on Amazon). Separate arrays for provider and method cannot represent co-occurrence correctly. Encoding `(provider_id, method_id)` into a single integer preserves the relationship and enables fast overlap filtering.

#### Encoding

* method ids: `stream=1`, `rent=2`, `buy=3`
* `watch_offer_key = (provider_id << 2) | method_id`

Then:

* “Amazon + rent” → `[encode(amazon, rent)]`
* “(Netflix OR Amazon) AND (stream OR rent)” → cross-product keys expanded in server code

---

## 3) Posting tables (the inverted indices)

### General purpose

Each posting table is an **inverted index** from `term_id → movie_id`, implemented as **row-per-posting**:

* one row per `(term_id, movie_id)` pair
* highly indexable
* joins cleanly against `eligible` (metadata filter results)
* allows counting distinct matched entities cheaply (`COUNT(DISTINCT term_id)`)

**Important:** `term_id` is always `lex.lexical_dictionary.string_id`.

---

### 3.1 `lex.inv_title_token_postings`

#### Purpose

Inverted index for **movie title tokens** (words + hyphen expansions). Used to compute:

* token match counts per movie for title scoring
* candidate generation from title searches (after score threshold)

#### Schema

```sql
CREATE TABLE lex.inv_title_token_postings (
  term_id  BIGINT NOT NULL,
  movie_id BIGINT NOT NULL,
  PRIMARY KEY (term_id, movie_id)
);

-- Future support: rebuild postings for a movie, deletes, etc.
CREATE INDEX idx_title_postings_movie
  ON lex.inv_title_token_postings (movie_id);
```

---

### 3.2 `lex.inv_person_postings`

#### Purpose

Inverted index for **people phrases**, defined as the union of:

* actors, directors, writers, composers, producers

No tokenization. Exact phrase matching only.

Used to compute:

* `matched_people_count = COUNT(DISTINCT term_id)` for requested people IDs

#### Schema

```sql
CREATE TABLE lex.inv_person_postings (
  term_id  BIGINT NOT NULL,
  movie_id BIGINT NOT NULL,
  PRIMARY KEY (term_id, movie_id)
);

CREATE INDEX idx_person_postings_movie
  ON lex.inv_person_postings (movie_id);
```

---

### 3.3 `lex.inv_character_postings`

#### Purpose

Inverted index for **character phrases** (exact phrase matching). Used to compute:

* `matched_character_count = COUNT(DISTINCT term_id)`

#### Schema

```sql
CREATE TABLE lex.inv_character_postings (
  term_id  BIGINT NOT NULL,
  movie_id BIGINT NOT NULL,
  PRIMARY KEY (term_id, movie_id)
);

CREATE INDEX idx_character_postings_movie
  ON lex.inv_character_postings (movie_id);
```

---

### 3.4 `lex.inv_studio_postings`

#### Purpose

Inverted index for **studio phrases** (production companies). Exact phrase matching. Used to compute:

* `matched_studio_count = COUNT(DISTINCT term_id)`

Studios are **unsorted** (no reception prior).

#### Schema

```sql
CREATE TABLE lex.inv_studio_postings (
  term_id  BIGINT NOT NULL,
  movie_id BIGINT NOT NULL,
  PRIMARY KEY (term_id, movie_id)
);

CREATE INDEX idx_studio_postings_movie
  ON lex.inv_studio_postings (movie_id);
```

---

## 4) Title token document frequency (max_df)

### 4.1 `lex.title_token_doc_frequency` (materialized view)

#### Purpose

Supports `max_df` filtering for title tokens:

* exclude extremely common title tokens (doc_frequency > 10,000)
* applies **only** to title tokens, not phrase buckets

This view also serves as an authoritative set of “strings that are actually used as title tokens”, which is important for restricting fuzzy matching to tokens.

#### Schema

```sql
CREATE MATERIALIZED VIEW lex.title_token_doc_frequency AS
SELECT
  term_id,
  COUNT(*)::BIGINT AS doc_frequency,
  now() AS updated_at
FROM lex.inv_title_token_postings
GROUP BY term_id;

CREATE UNIQUE INDEX idx_title_token_df_term_id
  ON lex.title_token_doc_frequency (term_id);
```

#### Refresh

When operationally ready:

```sql
REFRESH MATERIALIZED VIEW CONCURRENTLY lex.title_token_doc_frequency;
```

---

## 5) Optional reference tables (non-critical, debugging only)

These are not required for query execution because the server passes IDs, but they’re useful for introspection and admin tooling:

```sql
CREATE TABLE lex.genre_dictionary (
  genre_id INT PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);

CREATE TABLE lex.provider_dictionary (
  provider_id INT PRIMARY KEY,
  name TEXT NOT NULL UNIQUE
);

CREATE TABLE lex.watch_method_dictionary (
  method_id INT PRIMARY KEY,
  name TEXT NOT NULL UNIQUE  -- stream/rent/buy
);

CREATE TABLE lex.maturity_dictionary (
  maturity_rank SMALLINT PRIMARY KEY,
  label TEXT NOT NULL UNIQUE
);
```

---

## 6) How these tables support query logic (in one paragraph)

At query time, the server:

1. normalizes and resolves phrases to `term_id` via `lexical_dictionary` (exact for people/characters/studios)
2. expands title tokens via fuzzy lookup (trigram shortlist + levenshtein ≤ 1) **restricted to term_ids present in** `title_token_doc_frequency` and filtered by `max_df ≤ 10,000`
3. if any metadata filters exist, builds an `eligible(movie_id, title_token_count)` set from `lex.movies` using AND semantics
4. joins postings tables against `eligible` to count matched entities and compute title `m/k/L` stats
5. computes title scores (β=2, threshold=0.15), unions candidates (OR semantics), optionally anti-joins excluded phrase terms
6. returns component scores so the server can compute `raw` and normalize by `max_possible`

---

## 7) Design choices recap (why this structure)

* **Row-per-posting** instead of arrays: simplifies filtering, joining, counting matched entities, and future updates (`DELETE WHERE movie_id = …` works).
* **Global dictionary**: compact storage and fast equality lookup; required for consistent IDs across ingestion/query.
* **Materialized DF view**: enforces `max_df` and restricts fuzzy matching to title tokens, preventing fuzzy matches from drifting into phrase-only strings.
* **Metadata in `lex.movies`**: enables efficient eligible-set construction with B-tree + GIN indexes.
* **Encoded watch offers**: preserves provider↔method pairing so “Amazon + rent” is correct.
