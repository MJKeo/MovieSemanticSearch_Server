{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d76ce6aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/michaelkeohane/Documents/movie-finder-rag/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        " \n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, ConfigDict, field_validator, Field, RootModel\n",
        "from enum import Enum\n",
        "from typing import List, Optional, Dict, Tuple, Any\n",
        "from datetime import date\n",
        "from concurrent import futures\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from openai.lib._pydantic import to_strict_json_schema\n",
        "\n",
        "# Add parent directory to path to import from implementation package\n",
        "# Notebooks are in implementation/notebooks/, so we go up two levels to project root\n",
        "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
        "\n",
        "from implementation.llms.query_understanding_methods import (\n",
        "    extract_lexical_entities,\n",
        "    create_channel_weights,\n",
        "    extract_single_metadata_preference,\n",
        "    extract_all_metadata_preferences,\n",
        "    create_single_vector_subquery,\n",
        "    create_all_vector_subqueries,\n",
        "    create_single_vector_weight,\n",
        "    create_all_vector_weights,\n",
        "    create_overall_query_understanding,\n",
        ")\n",
        "\n",
        "# Load environment variables (for API key)\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fc797a6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "OVERALL_TEST_QUERIES = [\n",
        "    \"The Godfather\",\n",
        "    \"Tom Hanks movies\",\n",
        "    \"90s comedies\",\n",
        "    \"something feel-good and lighthearted\",\n",
        "    \"that movie with the spinning top at the end\",\n",
        "    \"films directed by Fincher starring Brad Pitt\",\n",
        "    \"A24 horror movies\",\n",
        "    \"movies with a character like Walter White\",\n",
        "    \"Spielberg and Lucas collaborations\",\n",
        "    \"that leandro dicaprio boat movie from 2001\",\n",
        "    \"Christoph Nolan's space movie with Matt Damon\",\n",
        "    \"shawshank movie prison escape morgan friedman\",\n",
        "    \"horror movies but not slashers or torture porn\",\n",
        "    \"80s action without Schwarzenegger or Stallone\",\n",
        "    \"thrillers that aren't too stressful or dark\",\n",
        "    \"critically acclaimed sci-fi from the last 5 years under 2 hours\",\n",
        "    \"R-rated crime dramas from before 1980\",\n",
        "    \"foreign language best picture nominees\",\n",
        "    \"something my parents and kids can all watch together\",\n",
        "    \"background movie while I work, nothing too demanding\",\n",
        "    \"first date movie that's romantic but not cheesy\",\n",
        "    \"movies about grief and learning to move on\",\n",
        "    \"heist movies with a twist ending where the villain wins\",\n",
        "    \"nonlinear storytelling like Pulp Fiction or Memento\",\n",
        "    \"underrated 90s neo-noir thrillers with morally ambiguous protagonists, preferably under 2 hours, not directed by the usual suspects like Tarantino\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a38792c",
      "metadata": {},
      "source": [
        "## Generic Kimmi K Calling Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd440025",
      "metadata": {},
      "source": [
        "## Lexical Entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2911fbae",
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_LEXICAL_QUERIES = [\n",
        "    \"movies starring Tom Hanks\",\n",
        "    \"films with Leandro Dicaprio and Bred Pit\",\n",
        "    \"best LOTR movies\",\n",
        "    \"action movies not starring Nicolas Cage\",\n",
        "    \"Pixar movies directed by Brad Bird starring Samuel L Jackson\",\n",
        "    \"movies like Star Wars\",\n",
        "    \"movies featuring Darth Vader\",\n",
        "    \"horror films from A24 or Blumhouse\",\n",
        "    \"movies with Jack Sparrow\",\n",
        "    \"funny scary movies from the 90s with great acting\",\n",
        "    \"rocky 4 and rambo 2\",\n",
        "    \"movies like se7en or the number 23\",\n",
        "    \"films with schwarzenegger and stallone\",\n",
        "    \"movies with the word 'death' in the title\",\n",
        "    \"waner brothers movies with harry poter not directed by chris colombus\",\n",
        "    \"best action movies with plot twists\",\n",
        "    \"romantic comedies from the 2000s\",\n",
        "    \"underrated foreign films with strong female leads\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "358f1bc5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing lexical entity queries: 100%|██████████| 25/25 [00:03<00:00,  7.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 25 lexical entity results\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run sample test cases (parallelized, up to 25 queries at a time)\n",
        "\n",
        "lexical_entity_results = []\n",
        "\n",
        "with futures.ThreadPoolExecutor(max_workers=25) as executor:\n",
        "    # Submit extract_lexical_entities for each query\n",
        "    future_to_query = {executor.submit(extract_lexical_entities, query): query for query in OVERALL_TEST_QUERIES}\n",
        "\n",
        "    # Collect results as they complete, with progress bar\n",
        "    for future in tqdm(\n",
        "        futures.as_completed(future_to_query),\n",
        "        total=len(OVERALL_TEST_QUERIES),\n",
        "        desc=\"Processing lexical entity queries\",\n",
        "    ):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            results = future.result()\n",
        "            lexical_entity_results.append((query, results))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query '{query}': {e}\")\n",
        "\n",
        "print(f\"Generated {len(lexical_entity_results)} lexical entity results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7909f0b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "lexical_entity_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1746fff5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 25 results to lexical_entity_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Save to lexical_entity_results.csv\n",
        "\n",
        "with open('../generated_data/lexical_entity_results.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    for query, results in lexical_entity_results:\n",
        "        writer.writerow({\"query\": query, \"results\": results.model_dump_json()})\n",
        "\n",
        "print(f\"Saved {len(lexical_entity_results)} results to lexical_entity_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a89e0c",
      "metadata": {},
      "source": [
        "## Metadata Preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "90a7bbd5",
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_METADATA_QUERIES = [\n",
        "    \"brisk 90s action flick, nothing plodding\",\n",
        "    \"Portuguese thriller with English subtitles, won something at Sundance\",\n",
        "    \"everyone saw it but critics were mixed, big summer tentpole\",\n",
        "    \"leisurely paced drama, I have all afternoon\",\n",
        "    \"Taiwanese coming-of-age, light and breezy, nothing heavy\",\n",
        "    \"something trashy and fun, totally panned, perfect for wine night\",\n",
        "    \"late 2010s superhero film, appropriate for my 12-year-old\",\n",
        "    \"moody Nordic noir, could be Swedish or Danish\",\n",
        "    \"tightly edited, under 100 minutes, no filler\",\n",
        "    \"arthouse darling that flopped commercially\",\n",
        "    \"streaming free on Tubi, campy 80s horror, the cheesier the better\",\n",
        "    \"beautifully shot but narratively messy, visually stunning\",\n",
        "    \"films from the silent era, slapstick preferred\",\n",
        "    \"I can only rent tonight, nothing on my subscriptions has what I want\",\n",
        "    \"British gangster film, stylish and quotable, Guy Ritchie vibes\",\n",
        "    \"not looking for anything mainstream, obscure foreign gems only\",\n",
        "    \"certified banger, everyone at work won't shut up about it\",\n",
        "    \"exactly rated R, I want the hard stuff, uncut\",\n",
        "    \"polarizing film, some call it genius others call it pretentious garbage\",\n",
        "    \"Australian outback thriller, gritty and relentless, 2000s era\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9bec8575",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing metadata preference queries: 100%|██████████| 20/20 [00:12<00:00,  1.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 20 responses\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "metadata_preferences_results = []\n",
        "\n",
        "with futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
        "    # Submit extract_lexical_entities for each query\n",
        "    future_to_query = {executor.submit(extract_all_metadata_preferences, query): query for query in TEST_METADATA_QUERIES}\n",
        "\n",
        "    # Collect results as they complete, with progress bar\n",
        "    for future in tqdm(\n",
        "        futures.as_completed(future_to_query),\n",
        "        total=len(TEST_METADATA_QUERIES),\n",
        "        desc=\"Processing metadata preference queries\",\n",
        "    ):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            results = future.result()\n",
        "            metadata_preferences_results.append((query, results))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query '{query}': {e}\")\n",
        "\n",
        "print(f\"Generated {len(metadata_preferences_results)} responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7f49dcbb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('brisk 90s action flick, nothing plodding',\n",
              "  MetadataPreferencesResponse(release_date_preference=DatePreference(first_date='1990-01-01', match_operation='between', second_date='1999-12-31'), duration_preference=NumericalPreference(first_value=90.0, match_operation='exact', second_value=None), genres_preference=GenreListPreference(should_include=['Action'], should_exclude=[]), audio_languages_preference=ListPreference(should_include=[], should_exclude=[]), watch_providers_preference=WatchProvidersPreference(should_include=[], should_exclude=[], preferred_access_type=None), maturity_rating_preference=MaturityPreference(rating='PG-13', match_operation='less_than_or_equal'), popular_trending_preference=PopularTrendingPreference(prefers_trending_movies=False, prefers_popular_movies=False), reception_preference=ReceptionPreference(reception_type='no_preference')))]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metadata_preferences_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "16a4d218",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 20 results to metadata_preferences_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Save to metadata_preferences_results.csv\n",
        "\n",
        "with open(\"../generated_data/metadata_preferences_results.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    for query, results in metadata_preferences_results:\n",
        "        writer.writerow({\"query\": query, \"results\": results.model_dump_json()})\n",
        "\n",
        "print(f\"Saved {len(metadata_preferences_results)} results to metadata_preferences_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d2d48d",
      "metadata": {},
      "source": [
        "## Vector Subqueries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "524f9245",
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_VECTOR_QUERIES = [\n",
        "  \"manly action movies from the 80s\",\n",
        "  \"movies like parasite but american and funnier not dumb\",\n",
        "  \"1990s french psych thriller not slow not frantic nonlinear timeline unreliable narrator no gore but creepy critics said beautiful cinematography plot holes\",\n",
        "  \"hand drawn animation not cgi spanish audio coming of age dramedy uplifting and hopeful iconic songs great dialogue\",\n",
        "  \"two strangers handcuffed together escape the city in one night set in tokyo time loop twist ending\",\n",
        "  \"date night movie to unwind after a long day funny but not gross no jump scares\",\n",
        "  \"low budget indie filmed in new york directed by nolan?? (or similar vibe) mixed reviews overrated but still smart\",\n",
        "  \"science fiction war epic intergalactic warfare morally gray lead ticking clock deadline red herrings\",\n",
        "  \"love-to-hate villain redemption arc but also unreliable narrator fourth wall breaks\",\n",
        "  # \"cozy sick day comfort watch background at a party not too loud not overstimulating ear bursting sound avoid that\",\n",
        "  \"a goofy movie but i mean goofy like silly not the title, 90s vibe, witty dialogue, not slow\",\n",
        "  \"romcom about two rival bakers, light and flirty, 00s vibe\",\n",
        "  # \"doc about free solo climbers, inspiring but not preachy\",\n",
        "  \"YA fantasy with a chosen one prophecy, not too dark, PG-13\",\n",
        "  \"set in Boston during a blizzard, but filmed in Toronto\",\n",
        "  \"something on Netflix under 90 minutes\",\n",
        "  \"critics hated it but I love it anyway, fun guilty pleasure\",\n",
        "  \"Oscar-winning cinematography, but the story is messy\",\n",
        "  \"real-time thriller in one apartment, ticking clock deadline, no flashbacks\",\n",
        "  \"found footage horror, no jump scares, creepy dread\",\n",
        "  # \"multiple POVs, unreliable narrator, twist ending explained at the end\",\n",
        "  \"movies with Jack Sparrow energy but not Pirates, witty swashbuckling\",\n",
        "  \"directed by Quinten Tarantino, snappy dialogue, violent but funny\",\n",
        "  \"Her but not the one with Joaquin Phoenix\",\n",
        "  \"ultra-gory body horror, disgusting, make me squirm\",\n",
        "  \"background while coding, dialogue not important, chill visuals, low volume\",\n",
        "  \"family movie night with kids, not babyish, jokes for adults too\",\n",
        "  \"Korean audio with English subtitles, critics called it a slow-burn masterpiece\",\n",
        "  \"adapted from a video game, big studio blockbuster, mixed reviews, amazing fight choreography\",\n",
        "  \"set in ancient Rome, political betrayal, ends on a bleak note\",\n",
        "  \"A24 vibe, but I want it less depressing, more hopeful\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bd6d9375",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing vector subquery speed: 100%|██████████| 7/7 [00:19<00:00,  2.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median: 2.83s\n",
            "Mean: 2.77s\n",
            "Mean justification length (chars): 66.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# TESTING HOW LONG IT TAKES TO GENERATE 7 RESPONSES\n",
        "\n",
        "import statistics\n",
        "\n",
        "times = []\n",
        "mean_justification_lengths = []\n",
        "\n",
        "for query in tqdm(TEST_VECTOR_QUERIES[:7], desc=\"Testing vector subquery speed\"):\n",
        "    start = time.perf_counter()\n",
        "    result = create_all_vector_subqueries(query=query)\n",
        "    elapsed = time.perf_counter() - start\n",
        "    times.append(elapsed)\n",
        "\n",
        "    # Track mean length of justification field across all pieces in the result\n",
        "    if result is not None:\n",
        "        justification_lengths = [\n",
        "            len(getattr(result.plot_events_data, \"justification\", \"\") or \"\"),\n",
        "            len(getattr(result.plot_analysis_data, \"justification\", \"\") or \"\"),\n",
        "            len(getattr(result.viewer_experience_data, \"justification\", \"\") or \"\"),\n",
        "            len(getattr(result.watch_context_data, \"justification\", \"\") or \"\"),\n",
        "            len(getattr(result.narrative_techniques_data, \"justification\", \"\") or \"\"),\n",
        "            len(getattr(result.production_data, \"justification\", \"\") or \"\"),\n",
        "            len(getattr(result.reception_data, \"justification\", \"\") or \"\"),\n",
        "        ]\n",
        "        mean_justification_lengths.append(statistics.mean(justification_lengths))\n",
        "\n",
        "print(f\"Median: {statistics.median(times):.2f}s\")\n",
        "print(f\"Mean: {statistics.mean(times):.2f}s\")\n",
        "print(f\"Mean justification length (chars): {statistics.mean(mean_justification_lengths):.1f}\" if mean_justification_lengths else \"No justifications to measure\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "dba1c97b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing vector subquery queries: 100%|██████████| 28/28 [00:12<00:00,  2.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 28 responses\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ACTUALLY GENERATING THE VECTOR SUBQUERIES\n",
        "\n",
        "vector_subquery_results = []\n",
        "\n",
        "with futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
        "    # Submit extract_lexical_entities for each query\n",
        "    future_to_query = {executor.submit(create_all_vector_subqueries, query): query for query in TEST_VECTOR_QUERIES}\n",
        "\n",
        "    # Collect results as they complete, with progress bar\n",
        "    for future in tqdm(\n",
        "        futures.as_completed(future_to_query),\n",
        "        total=len(TEST_VECTOR_QUERIES),\n",
        "        desc=\"Processing vector subquery queries\",\n",
        "    ):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            results = future.result()\n",
        "            vector_subquery_results.append((query, results))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query '{query}': {e}\")\n",
        "\n",
        "print(f\"Generated {len(vector_subquery_results)} responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "985bda66",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 28 results to vector_subquery_results.csv\n"
          ]
        }
      ],
      "source": [
        "# SAVE GENERATED VECTOR SUBQUERIES AS CSV\n",
        "\n",
        "with open('../generated_data/vector_subquery_results.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    for query, results in vector_subquery_results:\n",
        "        writer.writerow({\"query\": query, \"results\": results.model_dump_json()})\n",
        "\n",
        "print(f\"Saved {len(vector_subquery_results)} results to vector_subquery_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54758bc",
      "metadata": {},
      "source": [
        "## Vector Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ff9101d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing vector weights speed: 100%|██████████| 7/7 [00:10<00:00,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median: 1.47s\n",
            "Mean: 1.54s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# TESTING HOW LONG IT TAKES TO GENERATE 7 RESPONSES\n",
        "\n",
        "import statistics\n",
        "\n",
        "times = []\n",
        "for query in tqdm(TEST_VECTOR_QUERIES[:7], desc=\"Testing vector weights speed\"):\n",
        "    start = time.perf_counter()\n",
        "    create_all_vector_weights(query=query)\n",
        "    elapsed = time.perf_counter() - start\n",
        "    times.append(elapsed)\n",
        "\n",
        "print(f\"Median: {statistics.median(times):.2f}s\")\n",
        "print(f\"Mean: {statistics.mean(times):.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "24143090",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing vector weights queries: 100%|██████████| 28/28 [00:10<00:00,  2.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 28 responses\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process all queries in parallel (each query runs its prompts in parallel internally)\n",
        "\n",
        "vector_weights_results = []\n",
        "\n",
        "with futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
        "    # Submit extract_lexical_entities for each query\n",
        "    future_to_query = {executor.submit(create_all_vector_weights, query): query for query in TEST_VECTOR_QUERIES}\n",
        "\n",
        "    # Collect results as they complete, with progress bar\n",
        "    for future in tqdm(\n",
        "        futures.as_completed(future_to_query),\n",
        "        total=len(TEST_VECTOR_QUERIES),\n",
        "        desc=\"Processing vector weights queries\",\n",
        "    ):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            results = future.result()\n",
        "            vector_weights_results.append((query, results))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query '{query}': {e}\")\n",
        "\n",
        "print(f\"Generated {len(vector_weights_results)} responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "124e0d24",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 28 queries and saved results to vector_weights_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Write results to CSV file\n",
        "\n",
        "with open('../generated_data/vector_weights_results.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    for query, results in vector_weights_results:\n",
        "        writer.writerow({\"query\": query, \"results\": results.model_dump_json()})\n",
        "\n",
        "print(f\"Processed {len(vector_weights_results)} queries and saved results to vector_weights_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62403d0b",
      "metadata": {},
      "source": [
        "## Channel Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0c4b3262",
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_CHANNEL_WEIGHTS_QUERIES = [\n",
        "    \"Tom Hanks\",\n",
        "    \"movies from the 90s on Netflix under 2 hours\",\n",
        "    \"something cozy for a rainy Sunday that won't stress me out\",\n",
        "    \"Pixar movies that will make me cry\",\n",
        "    \"leandro dicaprio boat movie\",\n",
        "    \"The Matrix\",\n",
        "    \"critically acclaimed horror from the 2010s\",\n",
        "    \"movies with unreliable narrators and twist endings\",\n",
        "    \"trending action comedy\",\n",
        "    \"something like John Wick but less violent\",\n",
        "    \"French New Wave films\",\n",
        "    \"kids movies in Spanish\",\n",
        "    \"overrated superhero blockbusters\",\n",
        "    \"Brad Pitt and George Clooney heist movies from before 2010\",\n",
        "    \"disturbing psychological thrillers that mess with your head\",\n",
        "    \"not another Marvel movie, give me something like Nolan's work but more accessible and under 2 hours\",\n",
        "    \"that one where the city folds in on itself, not the animated one\",\n",
        "    \"prestige TV vibes but it's a movie, ensemble cast, slow burn\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1c9a8fce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing channel weights queries: 100%|██████████| 18/18 [00:01<00:00,  9.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 18 responses\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "channel_weights_results = []\n",
        "\n",
        "with futures.ThreadPoolExecutor(max_workers=25) as executor:\n",
        "    # Submit extract_lexical_entities for each query\n",
        "    future_to_query = {executor.submit(create_channel_weights, query): query for query in TEST_CHANNEL_WEIGHTS_QUERIES}\n",
        "\n",
        "    # Collect results as they complete, with progress bar\n",
        "    for future in tqdm(\n",
        "        futures.as_completed(future_to_query),\n",
        "        total=len(TEST_CHANNEL_WEIGHTS_QUERIES),\n",
        "        desc=\"Processing channel weights queries\",\n",
        "    ):\n",
        "        query = future_to_query[future]\n",
        "        try:\n",
        "            results = future.result()\n",
        "            channel_weights_results.append((query, results))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query '{query}': {e}\")\n",
        "\n",
        "print(f\"Generated {len(channel_weights_results)} responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f45be8b1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 28 queries and saved results to channel_weights_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Write results to CSV: query plus each response's relevance fields\n",
        "\n",
        "with open('../generated_data/channel_weights_results.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    for query, results in channel_weights_results:\n",
        "        writer.writerow({\"query\": query, \"results\": results.model_dump_json()})\n",
        "    \n",
        "print(f\"Processed {len(vector_weights_results)} queries and saved results to channel_weights_results.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43318b6a",
      "metadata": {},
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "354f1ca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SINGLE QUERY TEST\n",
        "\n",
        "query = OVERALL_TEST_QUERIES[0]\n",
        "\n",
        "overall = create_overall_query_understanding(\n",
        "    query=query\n",
        ")\n",
        "\n",
        "print(query)\n",
        "print(json.dumps(overall.model_dump(), indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0bc6a756",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing overall query speed: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Median: 2.39s\n",
            "Mean: 2.63s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# TESTING HOW LONG IT TAKES TO GENERATE 7 RESPONSES\n",
        "\n",
        "import statistics\n",
        "\n",
        "times = []\n",
        "for query in tqdm(OVERALL_TEST_QUERIES[:7], desc=\"Testing overall query speed\"):\n",
        "    start = time.perf_counter()\n",
        "    create_overall_query_understanding(query=query)\n",
        "    elapsed = time.perf_counter() - start\n",
        "    times.append(elapsed)\n",
        "\n",
        "print(f\"Median: {statistics.median(times):.2f}s\")\n",
        "print(f\"Mean: {statistics.mean(times):.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b333abc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing overall test queries:   0%|          | 0/25 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing overall test queries: 100%|██████████| 25/25 [03:34<00:00,  8.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 25 responses\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# PROCESS ALL TEST QUERIES (DO 8 PER MINUTE)\n",
        "\n",
        "MIN_SECONDS_PER_QUERY = 60 / 8  # 7.5 seconds between iterations (8 per 60 seconds)\n",
        "\n",
        "\n",
        "def _serialize_for_json(obj: Any) -> Any:\n",
        "    \"\"\"Convert Pydantic models and nested structures to JSON-serializable dicts.\"\"\"\n",
        "    if hasattr(obj, \"model_dump\"):\n",
        "        return obj.model_dump(mode=\"json\")\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: _serialize_for_json(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [_serialize_for_json(v) for v in obj]\n",
        "    return obj\n",
        "\n",
        "\n",
        "overall_results = []\n",
        "for query in tqdm(OVERALL_TEST_QUERIES, desc=\"Processing overall test queries\"):\n",
        "    start = time.perf_counter()\n",
        "    result = create_overall_query_understanding(query)\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    if result is None:\n",
        "        overall_results.append({\"query\": query, \"results\": json.dumps({\"error\": \"Query understanding failed\"})})\n",
        "    else:\n",
        "        # Build JSON-serializable dict compatible with Gradio viewer (vector_routing = vector_subqueries)\n",
        "        vw_serialized = _serialize_for_json(result.vector_weights)\n",
        "        # Gradio expects keys like \"plot_events\"; schema has \"plot_events_data\". Add aliases.\n",
        "        for key in list(vw_serialized.keys()):\n",
        "            if key.endswith(\"_data\"):\n",
        "                vw_serialized[key[:-5]] = vw_serialized[key]\n",
        "        serialized = {\n",
        "            \"channel_weights\": _serialize_for_json(result.channel_weights),\n",
        "            \"lexical_entities\": _serialize_for_json(result.lexical_entities),\n",
        "            \"metadata_preferences\": _serialize_for_json(result.metadata_preferences),\n",
        "            \"vector_routing\": _serialize_for_json(result.vector_subqueries),\n",
        "            \"vector_weights\": vw_serialized,\n",
        "        }\n",
        "        overall_results.append({\"query\": query, \"results\": json.dumps(serialized)})\n",
        "\n",
        "    # Rate limit: ensure at least MIN_SECONDS_PER_QUERY between iterations\n",
        "    sleep_time = MIN_SECONDS_PER_QUERY - elapsed\n",
        "    if sleep_time > 0:\n",
        "        time.sleep(sleep_time)\n",
        "\n",
        "print(f\"Generated {len(overall_results)} responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ea67f82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 25 results to /Users/michaelkeohane/Documents/movie-finder-rag/implementation/generated_data/overall_results.csv\n"
          ]
        }
      ],
      "source": [
        "csv_path = Path(\"../generated_data/overall_results.csv\")\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(overall_results)\n",
        "\n",
        "print(f\"Saved {len(overall_results)} results to {csv_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6676a992",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gradio interface for overall_results.csv\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _format_value(val, indent=0):\n",
        "    \"\"\"Recursively format a value for display (handles dicts, lists, None).\"\"\"\n",
        "    if val is None:\n",
        "        return \"_none_\"\n",
        "    if isinstance(val, bool):\n",
        "        return str(val)\n",
        "    if isinstance(val, (int, float)):\n",
        "        return str(val)\n",
        "    if isinstance(val, str):\n",
        "        return val\n",
        "    if isinstance(val, list):\n",
        "        if not val:\n",
        "            return \"[]\"\n",
        "        items = [_format_value(v, indent + 1) for v in val]\n",
        "        return \"\\n\" + \"  \" * (indent + 1) + (\"\\n\" + \"  \" * (indent + 1)).join(f\"- {x}\" for x in items)\n",
        "    if isinstance(val, dict):\n",
        "        lines = []\n",
        "        for k, v in val.items():\n",
        "            if v is None or v == \"\" or v == [] or v == {}:\n",
        "                continue\n",
        "            lines.append(f\"**{k.replace('_', ' ').title()}:** {_format_value(v, indent + 1)}\")\n",
        "        return \"\\n\" + \"  \" * (indent + 1) + (\"\\n\" + \"  \" * (indent + 1)).join(lines)\n",
        "    return str(val)\n",
        "\n",
        "\n",
        "def _format_metadata_preferences(mp: dict) -> str:\n",
        "    \"\"\"\n",
        "    Format metadata preferences as a bulleted list grouped by high-level keys.\n",
        "    Each top-level key (e.g., release_date_preference) becomes a bullet group\n",
        "    with its sub-items as nested bullets.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for key, val in mp.items():\n",
        "        label = key.replace(\"_\", \" \").title()\n",
        "        if val is None:\n",
        "            continue\n",
        "        if isinstance(val, dict):\n",
        "            sub_items = []\n",
        "            for k, v in val.items():\n",
        "                sub_label = k.replace(\"_\", \" \").title()\n",
        "                if isinstance(v, list):\n",
        "                    sub_val = \", \".join(str(x) for x in v) if v else \"(none)\"\n",
        "                    sub_items.append(f\"- **{sub_label}:** {sub_val}\")\n",
        "                elif v is None:\n",
        "                    sub_items.append(f\"- **{sub_label}:** (none)\")\n",
        "                else:\n",
        "                    sub_items.append(f\"- **{sub_label}:** {v}\")\n",
        "            if sub_items:\n",
        "                lines.append(f\"- **{label}**\")\n",
        "                lines.extend(f\"  {s}\" for s in sub_items)\n",
        "        elif isinstance(val, list):\n",
        "            lines.append(f\"- **{label}:** {', '.join(str(x) for x in val) if val else '(none)'}\")\n",
        "        else:\n",
        "            lines.append(f\"- **{label}:** {val}\")\n",
        "    return \"\\n\".join(lines) if lines else \"_No metadata preferences._\"\n",
        "\n",
        "\n",
        "def _escape_html(text: str) -> str:\n",
        "    \"\"\"Escape HTML special chars so content doesn't break tags.\"\"\"\n",
        "    return text.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace('\"', \"&quot;\")\n",
        "\n",
        "def _collapsible_section(title: str, content: str, open_by_default: bool = False) -> str:\n",
        "    \"\"\"Build HTML collapsible section for justifications.\"\"\"\n",
        "    open_attr = \" open\" if open_by_default else \"\"\n",
        "    safe_content = _escape_html(content)\n",
        "    return f'''\n",
        "<details{open_attr}>\n",
        "<summary><strong>{title}</strong></summary>\n",
        "<p style=\"margin: 0.5em 0; line-height: 1.5;\">{safe_content}</p>\n",
        "</details>\n",
        "'''\n",
        "\n",
        "\n",
        "def format_overall_results(query: str, df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Format the query results from overall_results.csv for display.\n",
        "    Justifications are placed in collapsible <details> sections.\n",
        "    \n",
        "    Args:\n",
        "        query: The selected query string\n",
        "        df: DataFrame loaded from overall_results.csv\n",
        "        \n",
        "    Returns:\n",
        "        HTML/Markdown string with formatted results\n",
        "    \"\"\"\n",
        "    row = df[df[\"query\"] == query]\n",
        "    if row.empty:\n",
        "        return \"Query not found.\"\n",
        "    \n",
        "    result_str = row.iloc[0][\"results\"]\n",
        "    try:\n",
        "        data = json.loads(result_str)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return f\"Error parsing results for query: {query}\"\n",
        "    \n",
        "    parts = [f\"# Query\\n\\n> **{query}**\\n\"]\n",
        "    \n",
        "    # Channel weights\n",
        "    cw = data.get(\"channel_weights\")\n",
        "    if cw is not None:\n",
        "        # Handle format: [query, dict] or just dict\n",
        "        weights = cw[1] if isinstance(cw, list) and len(cw) > 1 else (cw if isinstance(cw, dict) else {})\n",
        "        if weights:\n",
        "            parts.append(\"## Channel Weights\\n\")\n",
        "            parts.append(f\"- **Lexical relevance:** {weights.get('lexical_relevance', 'N/A')}\")\n",
        "            parts.append(f\"- **Metadata relevance:** {weights.get('metadata_relevance', 'N/A')}\")\n",
        "            parts.append(f\"- **Vector relevance:** {weights.get('vector_relevance', 'N/A')}\\n\")\n",
        "    \n",
        "    # Lexical entities\n",
        "    le = data.get(\"lexical_entities\", {})\n",
        "    entity_candidates = le.get(\"entity_candidates\", [])\n",
        "    parts.append(\"## Lexical Entities\\n\")\n",
        "    if entity_candidates:\n",
        "        for e in entity_candidates:\n",
        "            parts.append(f\"- **{e.get('corrected_and_normalized_entity', 'N/A')}** ({e.get('most_likely_category', '')})\")\n",
        "    else:\n",
        "        parts.append(\"_No entities extracted._\\n\")\n",
        "    \n",
        "    # Metadata preferences (bulleted list grouped by high-level keys)\n",
        "    mp = data.get(\"metadata_preferences\", {})\n",
        "    if mp:\n",
        "        parts.append(\"## Metadata Preferences\\n\")\n",
        "        parts.append(_format_metadata_preferences(mp) + \"\\n\")\n",
        "    \n",
        "    # Vectors: combined weight + subquery + justifications per collection\n",
        "    vr = data.get(\"vector_routing\", {}) or {}\n",
        "    vw = data.get(\"vector_weights\", {}) or {}\n",
        "    vector_collections = [\n",
        "        (\"plot_events_data\", \"plot_events\", \"Plot Events\"),\n",
        "        (\"plot_analysis_data\", \"plot_analysis\", \"Plot Analysis\"),\n",
        "        (\"viewer_experience_data\", \"viewer_experience\", \"Viewer Experience\"),\n",
        "        (\"watch_context_data\", \"watch_context\", \"Watch Context\"),\n",
        "        (\"narrative_techniques_data\", \"narrative_techniques\", \"Narrative Techniques\"),\n",
        "        (\"production_data\", \"production\", \"Production\"),\n",
        "        (\"reception_data\", \"reception\", \"Reception\"),\n",
        "    ]\n",
        "    if vr or vw:\n",
        "        parts.append(\"## Vectors\\n\")\n",
        "        for routing_key, weight_key, label in vector_collections:\n",
        "            routing_val = vr.get(routing_key)\n",
        "            weight_val = vw.get(weight_key)\n",
        "            if routing_val is None and weight_val is None:\n",
        "                continue\n",
        "            parts.append(f\"### {label}\\n\")\n",
        "            # Weight (relevance)\n",
        "            if isinstance(weight_val, str):\n",
        "                parts.append(f\"**Weight:** _Error: {weight_val}_\\n\")\n",
        "            elif isinstance(weight_val, dict):\n",
        "                relevance = weight_val.get(\"relevance\", \"N/A\")\n",
        "                weight_just = weight_val.get(\"justification\", \"\")\n",
        "                parts.append(f\"**Weight:** {relevance}\\n\")\n",
        "                if weight_just:\n",
        "                    parts.append(_collapsible_section(\"Weight justification\", weight_just))\n",
        "            else:\n",
        "                parts.append(\"**Weight:** N/A\\n\")\n",
        "            # Subquery\n",
        "            if isinstance(routing_val, dict):\n",
        "                subquery = routing_val.get(\"relevant_subquery_text\") or \"_none_\"\n",
        "                subquery_just = routing_val.get(\"justification\", \"\")\n",
        "                parts.append(f\"**Subquery:** `{subquery}`\\n\")\n",
        "                if subquery_just:\n",
        "                    parts.append(_collapsible_section(\"Subquery justification\", subquery_just))\n",
        "            elif routing_val is None and isinstance(weight_val, dict):\n",
        "                parts.append(\"**Subquery:** _none_\\n\")\n",
        "            parts.append(\"\\n\")\n",
        "    \n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "# Load overall_results.csv\n",
        "csv_path = Path(\"../generated_data/overall_results.csv\")\n",
        "if not csv_path.exists():\n",
        "    raise FileNotFoundError(f\"overall_results.csv not found at {csv_path.resolve()}\")\n",
        "\n",
        "df_overall = pd.read_csv(csv_path)\n",
        "query_choices = df_overall[\"query\"].tolist()\n",
        "\n",
        "# Build Gradio interface\n",
        "with gr.Blocks(title=\"Query Understanding Results\", theme=gr.themes.Soft()) as overall_interface:\n",
        "    gr.Markdown(\"# Query Understanding Results Viewer\")\n",
        "    gr.Markdown(\"Select a query to view its full extraction results. Justifications are in collapsible sections.\")\n",
        "    \n",
        "    query_dropdown = gr.Dropdown(\n",
        "        choices=query_choices,\n",
        "        value=query_choices[0] if query_choices else None,\n",
        "        label=\"Select Query\",\n",
        "        allow_custom_value=False,\n",
        "    )\n",
        "    \n",
        "    results_output = gr.Markdown(\n",
        "        value=format_overall_results(query_choices[0], df_overall) if query_choices else \"No data loaded.\"\n",
        "    )\n",
        "    \n",
        "    def on_query_change(query):\n",
        "        if not query:\n",
        "            return \"Select a query.\"\n",
        "        return format_overall_results(query, df_overall)\n",
        "    \n",
        "    query_dropdown.change(fn=on_query_change, inputs=[query_dropdown], outputs=[results_output])\n",
        "\n",
        "# Launch (use share=False for local only)\n",
        "overall_interface.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
