{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d76ce6aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/michaelkeohane/Documents/movie-finder-rag/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import csv\n",
        " \n",
        "import gradio as gr\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, ConfigDict, field_validator, Field, RootModel\n",
        "from enum import Enum\n",
        "from typing import List, Optional, Dict, Tuple, Any\n",
        "from datetime import date\n",
        "from concurrent import futures\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from openai.lib._pydantic import to_strict_json_schema\n",
        "\n",
        "# Add parent directory to path to import from implementation package\n",
        "# Notebooks are in implementation/notebooks/, so we go up two levels to project root\n",
        "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
        "\n",
        "from implementation.prompts.vector_subquery_prompts import (\n",
        "    VECTOR_QUERY_PROMPTS\n",
        ")\n",
        "from implementation.prompts.vector_weights_prompts import (\n",
        "    VECTOR_WEIGHT_PROMPTS\n",
        ")\n",
        "from implementation.prompts.metadata_preferences_prompts import (\n",
        "    ALL_METADATA_EXTRACTION_PROMPTS\n",
        ")\n",
        "from implementation.enums import Genre\n",
        "\n",
        "# Load environment variables (for API key)\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a38792c",
      "metadata": {},
      "source": [
        "## Generic Kimmi K Calling Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "72748b5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kimi API key loaded: sk-PCwbW...\n",
            "OpenAI API key loaded: sk-proj-...\n",
            "Successfully initialized both clients\n"
          ]
        }
      ],
      "source": [
        "kimi_api_key = os.environ.get(\"MOONSHOT_API_KEY\")\n",
        "if not kimi_api_key:\n",
        "    raise ValueError(\n",
        "        \"MOONSHOT_API_KEY environment variable not set. \"\n",
        "        \"Please set it before importing this module.\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"Kimi API key loaded: {kimi_api_key[:8]}...\")\n",
        "\n",
        "kimi_client = OpenAI(\n",
        "    api_key=kimi_api_key,\n",
        "    base_url=\"https://api.moonshot.ai/v1\",\n",
        ")\n",
        "\n",
        "# Get OpenAI API key from environment and initialize client once at module load\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY environment variable not set. \"\n",
        "        \"Please set it before importing this module.\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"OpenAI API key loaded: {openai_api_key[:8]}...\")\n",
        "\n",
        "# Initialize OpenAI client - created once when module is loaded\n",
        "openai_client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "print(\"Successfully initialized both clients\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6028342e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_openai_response(\n",
        "    user_prompt: str,\n",
        "    system_prompt: str,\n",
        "    response_format: BaseModel,\n",
        "    model: str = \"gpt-5-mini\",\n",
        "    reasoning_effort: str = \"low\",\n",
        "    verbosity: str = \"low\"\n",
        "):\n",
        "    response = openai_client.chat.completions.parse(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        response_format=response_format,\n",
        "        reasoning_effort=reasoning_effort,\n",
        "        verbosity=verbosity\n",
        "    )\n",
        "    \n",
        "    # Extract the parsed response - OpenAI automatically validates structure matches PlotMetadata\n",
        "    message = response.choices[0].message\n",
        "    if message.parsed:\n",
        "        return message.parsed\n",
        "    else:\n",
        "        # Handle case where model refuses to generate output\n",
        "        raise ValueError(f\"OpenAI failed to generate response: {message.refusal}\")\n",
        "\n",
        "def generate_kimi_response(\n",
        "    user_prompt: str,\n",
        "    system_prompt: str,\n",
        "    response_format: BaseModel,\n",
        "    enable_thinking: bool = False,\n",
        "):\n",
        "    try:\n",
        "        thinking_type = \"enabled\" if enable_thinking else \"disabled\"\n",
        "        schema = to_strict_json_schema(response_format)\n",
        "\n",
        "        response = kimi_client.chat.completions.create(\n",
        "            model=\"kimi-k2.5\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            response_format={\n",
        "                \"type\": \"json_schema\",\n",
        "                \"json_schema\": {\n",
        "                    \"name\": \"test_metadata\",\n",
        "                    \"strict\": True,\n",
        "                    \"schema\": schema,\n",
        "                },\n",
        "            },\n",
        "            extra_body={\n",
        "                \"thinking\": {\"type\": thinking_type}\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        # Extract the parsed response - OpenAI automatically validates structure matches PlotMetadata\n",
        "        raw = response.choices[0].message.content\n",
        "        data = json.loads(raw)\n",
        "        metadata = response_format.model_validate(data)\n",
        "        return metadata\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Kimi failed to generate response: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5c6ea0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestMetadata(BaseModel):\n",
        "    favorite_color: str\n",
        "    favorite_number: float\n",
        "\n",
        "response = generate_kimi_response(\n",
        "    user_prompt=\"Give me your favorite color and number\",\n",
        "    system_prompt=\"You are a helpful assistant that can answer questions and help with tasks. Your answer must follow the provided JSON schema.\",\n",
        "    response_format=TestMetadata\n",
        ")\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e260ae0c",
      "metadata": {},
      "source": [
        "## Test Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "112d5014",
      "metadata": {},
      "outputs": [],
      "source": [
        "routing_queries = [\n",
        "  \"manly action movies from the 80s\",\n",
        "  \"movies like parasite but american and funnier not dumb\",\n",
        "  \"1990s french psych thriller not slow not frantic nonlinear timeline unreliable narrator no gore but creepy critics said beautiful cinematography plot holes\",\n",
        "  \"hand drawn animation not cgi spanish audio coming of age dramedy uplifting and hopeful iconic songs great dialogue\",\n",
        "  \"two strangers handcuffed together escape the city in one night set in tokyo time loop twist ending\",\n",
        "  \"date night movie to unwind after a long day funny but not gross no jump scares\",\n",
        "  \"low budget indie filmed in new york directed by nolan?? (or similar vibe) mixed reviews overrated but still smart\",\n",
        "  \"science fiction war epic intergalactic warfare morally gray lead ticking clock deadline red herrings\",\n",
        "  \"love-to-hate villain redemption arc but also unreliable narrator fourth wall breaks\",\n",
        "  \"cozy sick day comfort watch background at a party not too loud not overstimulating ear bursting sound avoid that\",\n",
        "  \"a goofy movie but i mean goofy like silly not the title, 90s vibe, witty dialogue, not slow\",\n",
        "  \"romcom about two rival bakers, light and flirty, 00s vibe\",\n",
        "  \"doc about free solo climbers, inspiring but not preachy\",\n",
        "  \"YA fantasy with a chosen one prophecy, not too dark, PG-13\",\n",
        "  \"set in Boston during a blizzard, but filmed in Toronto\",\n",
        "  \"something on Netflix under 90 minutes\",\n",
        "  \"critics hated it but I love it anyway, fun guilty pleasure\",\n",
        "  \"Oscar-winning cinematography, but the story is messy\",\n",
        "  \"real-time thriller in one apartment, ticking clock deadline, no flashbacks\",\n",
        "  \"found footage horror, no jump scares, creepy dread\",\n",
        "  \"multiple POVs, unreliable narrator, twist ending explained at the end\",\n",
        "  \"movies with Jack Sparrow energy but not Pirates, witty swashbuckling\",\n",
        "  \"directed by Quinten Tarantino, snappy dialogue, violent but funny\",\n",
        "  \"Her but not the one with Joaquin Phoenix\",\n",
        "  \"ultra-gory body horror, disgusting, make me squirm\",\n",
        "  \"background while coding, dialogue not important, chill visuals, low volume\",\n",
        "  \"family movie night with kids, not babyish, jokes for adults too\",\n",
        "  \"Korean audio with English subtitles, critics called it a slow-burn masterpiece\",\n",
        "  \"adapted from a video game, big studio blockbuster, mixed reviews, amazing fight choreography\",\n",
        "  \"set in ancient Rome, political betrayal, ends on a bleak note\",\n",
        "  \"A24 vibe, but I want it less depressing, more hopeful\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd440025",
      "metadata": {},
      "source": [
        "## Lexical Entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "06cad525",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EntityCategory(Enum):\n",
        "    \"\"\"Enum representing the various categories of lexical entities.\"\"\"\n",
        "    CHARACTER = \"character\"\n",
        "    FRANCHISE = \"franchise\"\n",
        "    MOVIE_TITLE = \"movie_title\"\n",
        "    PERSON = \"person\"\n",
        "    STUDIO = \"studio\"\n",
        "\n",
        "\n",
        "class ExtractedEntity(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "    \n",
        "    candidate_entity_phrase: str\n",
        "    most_likely_category: EntityCategory\n",
        "    exclude_from_results: bool\n",
        "    corrected_and_normalized_entity: str\n",
        "\n",
        "class ExtractedEntities(BaseModel):\n",
        "    entity_candidates: List[ExtractedEntity]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c0c24ee7",
      "metadata": {},
      "outputs": [],
      "source": [
        "EXTRACT_LEXICAL_ENTITIES_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are an expert at understanding movie search queries. Your job is to extract all \\\n",
        "lexical entities from the provided search query.\n",
        "\n",
        "GOALS:\n",
        "- Identify all lexical entities contained within the search query\n",
        "- Correctly categorize each lexical entity (movie title, person, character, studio)\n",
        "- Normalize each lexical entity to its canonical form\n",
        "\n",
        "INPUT:\n",
        "You will receive text representing the full movie search query entered by the user.\n",
        "\n",
        "OUTPUT:\n",
        "JSON schema. A list of JSON objects, each representing a single lexical entity.\n",
        "- candidate_entity_phrase: The original verbatim word / phrase from the search query that represents a single lexical entity.\n",
        "- most_likely_category: The category that best represents this lexical entity.\n",
        "- exclude_from_results: Whether the user is trying to find movies that contain this entity or DON'T contain this entity (ex. \"Not starring Tom Cruise\" means DON'T contain Tom Cruise).\n",
        "- corrected_and_normalized_entity: The MOST LIKELY corrected and normalized form of the typed entity. Represents how that entity would appear on an official movie website or movie poster.\n",
        "\n",
        "ENTITY CATEGORIES:\n",
        "- movie_title: Represents a substring or the entirety of a SPECIFIC movie title.\n",
        "  - Case #1: The query contains a word or phrase that clearly and obviously is the title of a movie. (ex. \"shawshank redemption\", \"fight club\", \"movies like dark knight\")\n",
        "  - Case #2: In the query the user is explicitly searching for movies with a given substring in the title (ex. \"movies with the word 'clown' in the title)\n",
        "- franchise: Represents a specific media brand (ex. \"The Matrix\", \"Spongebob Squarepants\", \"Barbie\")\n",
        "- person: Represents the name of a real human who worked on this movie (actor, writer, composer, etc.).\n",
        "- character: Represents the name of a character who appears in this movie.\n",
        "- studio: Represents the name of a movie studio that produced this movie.\n",
        "\n",
        "CORRECTIONS & NORMALIZATIONS:\n",
        "- HIGH-CONFIDENCE (>95%) terms only\n",
        "- Clear spelling mistakes (ex. \"Leandro Dicaprio\" -> \"Leonardo DiCaprio\")\n",
        "- Normalized punctuation and numerical formats (ex. \"rocky 2\" --> \"rocky ii\", \"seven\" --> \"se7en\")\n",
        "- Obvious acronym expansions (ex. \"LOTR\" -> \"Lord of the Rings\")\n",
        "- NEVER introduce additional information not already present in the original query (ex. \"star wars\" -> \"Star Wars: Episode IV - \\\n",
        "A New Hope\" is BAD because the user never specified which specific Star Wars movie they are looking for)\n",
        "- Introducing additional information not present in the original query is a catastrophic failure.\n",
        "\n",
        "ADDITIONAL GUIDANCE:\n",
        "- All values must be nonnull. Providing a null value is a catastrophic failure. Providing None as a value is a catastrophic failure.\n",
        "- most_likely_category MUST be \"movie_title\", \"franchise\", \"person\", \"character\", or \"studio\"\n",
        "- corrected_and_normalized_entity must be the highest confidence correction / normalization of the user-typed entity.\n",
        "- Only extract words or phrases that are highly likely to be a lexical entity.\n",
        "- DO NOT extract words or phrases that simply describe traits of the movie. They MUST be related to specific lexical entities.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "9a9f6780",
      "metadata": {},
      "outputs": [],
      "source": [
        "entity_query = \"john wick type movies\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "id": "358f1bc5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ExtractedEntities(entity_candidates=[ExtractedEntity(candidate_entity_phrase='john wick', most_likely_category='franchise', exclude_from_results=False, corrected_and_normalized_entity='John Wick')])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kimi_response = generate_kimi_response(\n",
        "    user_prompt=f\"User query: \\\"{entity_query}\\\"\",\n",
        "    system_prompt=EXTRACT_LEXICAL_ENTITIES_SYSTEM_PROMPT,\n",
        "    response_format=ExtractedEntities,\n",
        ")\n",
        "\n",
        "kimi_response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07a89e0c",
      "metadata": {},
      "source": [
        "## Metadata Preferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "669cd171",
      "metadata": {},
      "outputs": [],
      "source": [
        "class DateMatchOperation(Enum):\n",
        "    EXACT = \"exact\"\n",
        "    BEFORE = \"before\"\n",
        "    AFTER = \"after\"\n",
        "    BETWEEN = \"between\"\n",
        "\n",
        "class NumericalMatchOperation(Enum):\n",
        "    EXACT = \"exact\"\n",
        "    BETWEEN = \"between\"\n",
        "    LESS_THAN = \"less_than\"\n",
        "    GREATER_THAN = \"greater_than\"\n",
        "\n",
        "class RatingMatchOperation(Enum):\n",
        "    EXACT = \"exact\"\n",
        "    GREATER_THAN = \"greater_than\"\n",
        "    LESS_THAN = \"less_than\"\n",
        "    GREATER_THAN_OR_EQUAL = \"greater_than_or_equal\"\n",
        "    LESS_THAN_OR_EQUAL = \"less_than_or_equal\"\n",
        "\n",
        "class RatingPreference(Enum):\n",
        "    CRITICALLY_ACCLAIMED = \"critically_acclaimed\"\n",
        "    POORLY_RECEIVED = \"poorly_received\"\n",
        "\n",
        "class StreamingAccessType(Enum):\n",
        "    SUBSCRIPTION = \"subscription\"\n",
        "    RENT = \"rent\"\n",
        "    BUY = \"buy\"\n",
        "\n",
        "\n",
        "\n",
        "class DatePreference(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "\n",
        "    first_date: str = Field(\n",
        "        ..., \n",
        "        pattern=r\"^\\d{4}-\\d{2}-\\d{2}$\",\n",
        "        description=\"Either the first date in the range or the exact date to match. ISO 8601 date: YYYY-MM-DD\",\n",
        "    )\n",
        "    match_operation: DateMatchOperation = Field(..., description=\"Whether we want the date to be before, after, or exactly at the first date, or between the two provided dates.\")\n",
        "    second_date: Optional[str] = Field(\n",
        "        default=None, \n",
        "        pattern=r\"^\\d{4}-\\d{2}-\\d{2}$\",\n",
        "        description=\"Optional second date in the range only if match_operation is BETWEEN. ISO 8601 date: YYYY-MM-DD\"\n",
        "    )\n",
        "    \n",
        "\n",
        "class NumericalPreference(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "\n",
        "    first_value: float = Field(..., description=\"Either the first value in the range or the exact value to match.\")\n",
        "    match_operation: NumericalMatchOperation = Field(..., description=\"How we should evaluate the provided first and (maybe) second values.\")\n",
        "    second_value: Optional[float] = Field(default=None, description=\"Optional second value in the range only if match_operation is BETWEEN.\")\n",
        "\n",
        "\n",
        "class ListPreference(BaseModel):\n",
        "    should_include: List[str] = Field(default=[], description=\"List of items that should be included in the movie's metadata.\")\n",
        "    should_exclude: List[str] = Field(default=[], description=\"List of items that should be excluded from the movie's metadata.\")\n",
        "\n",
        "\n",
        "class GenreListPreference(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "\n",
        "    should_include: List[Genre] = Field(default=[], description=\"List of genres that the user's query wants the movie to fall under.\")\n",
        "    should_exclude: List[Genre] = Field(default=[], description=\"List of genres that the user's query wants to avoid in the movie.\")\n",
        "\n",
        "\n",
        "class MaturityPreference(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "\n",
        "    rating: str = Field(..., description=\"Standard USA ratings: G, PG, PG-13, R, NC-17\")\n",
        "    match_operation: RatingMatchOperation = Field(..., description=\"Whether we prefer movies with this rating, greater (more mature), or less (less mature).\")\n",
        "\n",
        "\n",
        "class PopularTrendingPreference(BaseModel):\n",
        "    prefers_trending_movies: bool\n",
        "    prefers_popular_movies: bool\n",
        "\n",
        "\n",
        "class WatchProvidersPreference(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "    \n",
        "    should_include: List[str]\n",
        "    should_exclude: List[str]\n",
        "    preferred_access_type: Optional[StreamingAccessType]\n",
        "\n",
        "\n",
        "\n",
        "class MetadataPreferences(BaseModel):\n",
        "    release_date_preference: Optional[DatePreference]\n",
        "    duration_preference: Optional[NumericalPreference]\n",
        "    genres_preference: Optional[GenreListPreference]\n",
        "    audio_languages_preference: Optional[ListPreference]\n",
        "    watch_providers_preference: Optional[WatchProvidersPreference]\n",
        "    maturity_rating_preference: Optional[MaturityPreference]\n",
        "    popular_trending_preference: PopularTrendingPreference\n",
        "    rating_preference: Optional[RatingPreference]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "44fa0217",
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata_queries = [\n",
        "    \"brisk 90s action flick, nothing plodding\",\n",
        "    \"Portuguese thriller with English subtitles, won something at Sundance\",\n",
        "    \"everyone saw it but critics were mixed, big summer tentpole\",\n",
        "    \"leisurely paced drama, I have all afternoon\",\n",
        "    \"Taiwanese coming-of-age, light and breezy, nothing heavy\",\n",
        "    \"something trashy and fun, totally panned, perfect for wine night\",\n",
        "    \"late 2010s superhero film, appropriate for my 12-year-old\",\n",
        "    \"moody Nordic noir, could be Swedish or Danish\",\n",
        "    \"tightly edited, under 100 minutes, no filler\",\n",
        "    \"arthouse darling that flopped commercially\",\n",
        "    \"streaming free on Tubi, campy 80s horror, the cheesier the better\",\n",
        "    \"beautifully shot but narratively messy, visually stunning\",\n",
        "    \"films from the silent era, slapstick preferred\",\n",
        "    \"I can only rent tonight, nothing on my subscriptions has what I want\",\n",
        "    \"British gangster film, stylish and quotable, Guy Ritchie vibes\",\n",
        "    \"not looking for anything mainstream, obscure foreign gems only\",\n",
        "    \"certified banger, everyone at work won't shut up about it\",\n",
        "    \"exactly rated R, I want the hard stuff, uncut\",\n",
        "    \"polarizing film, some call it genius others call it pretentious garbage\",\n",
        "    \"Australian outback thriller, gritty and relentless, 2000s era\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bec8575",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing metadata preferences: 100%|██████████| 20/20 [00:30<00:00,  1.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 20 results to /Users/michaelkeohane/Documents/movie-finder-rag/implementation/notebooks/metadata_preferences_results.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Wrapper models for optional metadata preferences. Use BaseModel with value: Optional[X]\n",
        "# so the schema explicitly allows null. RootModel[Optional[T]] + json_schema_extra can\n",
        "# produce schemas that OpenAI strict mode treats as non-optional (e.g. anyOf at root).\n",
        "# These wrappers produce {\"value\": null} or {\"value\": {...}} - clean and reliably optional.\n",
        "class ReleaseDateResponse(BaseModel):\n",
        "    value: Optional[DatePreference] = None\n",
        "\n",
        "class DurationResponse(BaseModel):\n",
        "    value: Optional[NumericalPreference] = None\n",
        "\n",
        "class GenresResponse(BaseModel):\n",
        "    value: Optional[GenreListPreference] = None\n",
        "\n",
        "class AudioLanguagesResponse(BaseModel):\n",
        "    value: Optional[ListPreference] = None\n",
        "\n",
        "class WatchProvidersResponse(BaseModel):\n",
        "    value: Optional[WatchProvidersPreference] = None\n",
        "\n",
        "class MaturityRatingResponse(BaseModel):\n",
        "    value: Optional[MaturityPreference] = None\n",
        "\n",
        "class RatingResponse(BaseModel):\n",
        "    value: Optional[RatingPreference] = None\n",
        "\n",
        "# Prompt prefix for optional fields: prompts say \"Return null\" but we need {\"value\": null}\n",
        "OPTIONAL_RESPONSE_PREFIX = (\n",
        "    \"IMPORTANT: Your response must be a JSON object with a single 'value' key. \"\n",
        "    \"When no preference applies, use {\\\"value\\\": null}. \"\n",
        "    \"When you have a preference, use {\\\"value\\\": { ... your extraction ... }}.\\n\\n\"\n",
        ")\n",
        "\n",
        "# Mapping: (prompt_key, field_name, response_schema, needs_optional_prefix)\n",
        "METADATA_PREFERENCE_MAPPING = [\n",
        "    (\"release_date\", \"release_date_preference\", ReleaseDateResponse, True),\n",
        "    (\"duration\", \"duration_preference\", DurationResponse, True),\n",
        "    (\"genres\", \"genres_preference\", GenresResponse, True),\n",
        "    (\"audio_languages\", \"audio_languages_preference\", AudioLanguagesResponse, True),\n",
        "    (\"watch_providers\", \"watch_providers_preference\", WatchProvidersResponse, True),\n",
        "    (\"maturity_rating\", \"maturity_rating_preference\", MaturityRatingResponse, True),\n",
        "    (\"popularity\", \"popular_trending_preference\", PopularTrendingPreference, False),\n",
        "    (\"rating\", \"rating_preference\", RatingResponse, True),\n",
        "]\n",
        "\n",
        "\n",
        "def _process_single_metadata_preference(\n",
        "    prompt_key: str,\n",
        "    system_prompt: str,\n",
        "    response_schema: type,\n",
        "    needs_optional_prefix: bool,\n",
        "    query: str,\n",
        ") -> tuple[str, Any]:\n",
        "    \"\"\"\n",
        "    Process a single metadata preference extraction for a query.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (MetadataPreferences field name, extracted value or None)\n",
        "    \"\"\"\n",
        "    field_name = next(fn for pk, fn, _, _ in METADATA_PREFERENCE_MAPPING if pk == prompt_key)\n",
        "    try:\n",
        "        full_system_prompt = (\n",
        "            (OPTIONAL_RESPONSE_PREFIX + system_prompt) if needs_optional_prefix else system_prompt\n",
        "        )\n",
        "        response = generate_openai_response(\n",
        "            user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "            system_prompt=full_system_prompt,\n",
        "            response_format=response_schema,\n",
        "            model=\"gpt-5-mini\",\n",
        "            reasoning_effort=\"minimal\",\n",
        "        )\n",
        "        # response = generate_kimi_response(\n",
        "        #     user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "        #     system_prompt=full_system_prompt,\n",
        "        #     response_format=response_schema,\n",
        "        # )\n",
        "        # Optional wrappers have .value; PopularTrendingPreference is the model directly\n",
        "        if hasattr(response, \"value\"):\n",
        "            value = response.value\n",
        "        else:\n",
        "            value = response\n",
        "        return field_name, value\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {prompt_key} for query: {e}\")\n",
        "        return field_name, None\n",
        "\n",
        "\n",
        "def get_metadata_preferences_parallel(query: str) -> MetadataPreferences:\n",
        "    \"\"\"\n",
        "    Extract all metadata preferences for a query by running each prompt in parallel,\n",
        "    then combine into a single MetadataPreferences instance.\n",
        "    \"\"\"\n",
        "    with futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        future_to_key = {\n",
        "            executor.submit(\n",
        "                _process_single_metadata_preference,\n",
        "                prompt_key,\n",
        "                ALL_METADATA_EXTRACTION_PROMPTS[prompt_key],\n",
        "                response_schema,\n",
        "                needs_optional_prefix,\n",
        "                query,\n",
        "            ): prompt_key\n",
        "            for prompt_key, _, response_schema, needs_optional_prefix in METADATA_PREFERENCE_MAPPING\n",
        "        }\n",
        "        results = {}\n",
        "        for future in futures.as_completed(future_to_key):\n",
        "            field_name, value = future.result()\n",
        "            results[field_name] = value\n",
        "    return MetadataPreferences(\n",
        "        release_date_preference=results.get(\"release_date_preference\"),\n",
        "        duration_preference=results.get(\"duration_preference\"),\n",
        "        genres_preference=results.get(\"genres_preference\"),\n",
        "        audio_languages_preference=results.get(\"audio_languages_preference\"),\n",
        "        watch_providers_preference=results.get(\"watch_providers_preference\"),\n",
        "        maturity_rating_preference=results.get(\"maturity_rating_preference\"),\n",
        "        popular_trending_preference=results.get(\"popular_trending_preference\") or PopularTrendingPreference(\n",
        "            prefers_trending_movies=False, prefers_popular_movies=False\n",
        "        ),\n",
        "        rating_preference=results.get(\"rating_preference\"),\n",
        "    )\n",
        "\n",
        "\n",
        "metadata_preferences_results = []\n",
        "for query in tqdm(metadata_queries, desc=\"Processing metadata preferences\"):\n",
        "    try:\n",
        "        results = get_metadata_preferences_parallel(query)\n",
        "        metadata_preferences_results.append((query, results))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing query '{query}': {e}\")\n",
        "\n",
        "# Save to metadata_preferences_results.csv\n",
        "csv_path = Path(\"../generated_data/metadata_preferences_results.csv\")\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    for query, results in metadata_preferences_results:\n",
        "        writer.writerow({\"query\": query, \"results\": results.model_dump_json()})\n",
        "\n",
        "print(f\"Saved {len(metadata_preferences_results)} results to {csv_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "id": "27aef7f0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "release_date_preference: {'first_date': '1980-01-01', 'match_operation': 'between', 'second_date': '1989-12-31'}\n",
            "duration_preference: {'first_value': 90.0, 'match_operation': 'greater_than', 'second_value': None}\n",
            "genres_preference: {'must_include': ['Action'], 'must_exclude': []}\n",
            "audio_languages_preference: {'must_include': [], 'must_exclude': []}\n",
            "watch_providers_preference: {'must_include': [], 'must_exclude': []}\n",
            "maturity_rating_preference: {'rating': 'R', 'match_operation': 'less_than_or_equal'}\n"
          ]
        }
      ],
      "source": [
        "# print(json.dumps(kimi_response.model_dump(), indent=4))\n",
        "for k,v in kimi_response.model_dump().items():\n",
        "    if v:\n",
        "        print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d2d48d",
      "metadata": {},
      "source": [
        "## Vector Subqueries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e8f0ae19",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VectorCollectionQueryData(BaseModel):\n",
        "    justification: str\n",
        "    relevant_subquery_text: Optional[str]\n",
        "\n",
        "class VectorRoutingResponse(BaseModel):\n",
        "    plot_events_data: VectorCollectionQueryData\n",
        "    plot_analysis_data: VectorCollectionQueryData\n",
        "    viewer_experience_data: VectorCollectionQueryData\n",
        "    watch_context_data: VectorCollectionQueryData\n",
        "    narrative_techniques_data: VectorCollectionQueryData\n",
        "    production_data: VectorCollectionQueryData\n",
        "    reception_data: VectorCollectionQueryData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba1c97b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ACTUALLY GENERATING THE VECTOR SUBQUERIES\n",
        "\n",
        "responses = []\n",
        "\n",
        "def get_vector_queries(query):\n",
        "    queries = [\n",
        "        (VECTOR_QUERY_PROMPTS[\"plot_events\"], \"plot_events_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"plot_analysis\"], \"plot_analysis_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"viewer_experience\"], \"viewer_experience_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"watch_context\"], \"watch_context_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"narrative_techniques\"], \"narrative_techniques_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"production\"], \"production_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"reception\"], \"reception_data\"),\n",
        "    ]\n",
        "\n",
        "    def process_query(args):\n",
        "        system_prompt, key = args\n",
        "        print(f\"Generating for {key}...\")\n",
        "        try:\n",
        "            response = generate_kimi_response(\n",
        "                user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "                system_prompt=system_prompt,\n",
        "                response_format=VectorCollectionQueryData\n",
        "            )\n",
        "            return key, response\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {key}: {e}\")\n",
        "            return key, None\n",
        "\n",
        "    # Run queries in parallel\n",
        "    results = {}\n",
        "    with futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
        "        future_to_key = {executor.submit(process_query, item): item[1] for item in queries}\n",
        "        for future in futures.as_completed(future_to_key):\n",
        "            key, response = future.result()\n",
        "            if response:\n",
        "                results[key] = response\n",
        "\n",
        "    # Construct response from parallel results\n",
        "    return VectorRoutingResponse(\n",
        "        plot_events_data=results.get(\"plot_events_data\"),\n",
        "        plot_analysis_data=results.get(\"plot_analysis_data\"),\n",
        "        viewer_experience_data=results.get(\"viewer_experience_data\"),\n",
        "        watch_context_data=results.get(\"watch_context_data\"),\n",
        "        narrative_techniques_data=results.get(\"narrative_techniques_data\"),\n",
        "        production_data=results.get(\"production_data\"),\n",
        "        reception_data=results.get(\"reception_data\")\n",
        "    )\n",
        "\n",
        "for query in tqdm(routing_queries):\n",
        "    vector_response = get_vector_queries(query)\n",
        "    responses.append(vector_response)\n",
        "\n",
        "print(f\"Generated {len(responses)} responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985bda66",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 31 results to results.csv\n"
          ]
        }
      ],
      "source": [
        "# SAVE GENERATED VECTOR SUBQUERIES AS CSV\n",
        "\n",
        "with open('../generated_data/results.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    # Create CSV writer\n",
        "    fieldnames = ['query', 'result']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    \n",
        "    # Write header row\n",
        "    writer.writeheader()\n",
        "    \n",
        "    # Write data rows\n",
        "    for i, response in enumerate(responses):\n",
        "        row = {\n",
        "            'query': routing_queries[i],\n",
        "            'result': response.model_dump_json()\n",
        "        }\n",
        "        \n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Saved {len(responses)} results to results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "254a513f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# [GRADIO INTERFACE] Visualize vector subquery results\n",
        "\n",
        "\n",
        "COLLECTION_LABELS = {\n",
        "    \"plot_events_data\": \"Plot Events\",\n",
        "    \"plot_analysis_data\": \"Plot Analysis\",\n",
        "    \"viewer_experience_data\": \"Viewer Experience\",\n",
        "    \"watch_context_data\": \"Watch Context\",\n",
        "    \"narrative_techniques_data\": \"Narrative Techniques\",\n",
        "    \"production_data\": \"Production\",\n",
        "    \"reception_data\": \"Reception\",\n",
        "}\n",
        "\n",
        "def load_results_from_csv(filename: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Load query results from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the CSV file with columns 'query' and 'json_result' (or 'result')\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping query strings to their raw JSON result strings\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                query = row.get('query', '').strip()\n",
        "                # Handle both 'json_result' and 'result' column names\n",
        "                json_result = row.get('json_result', row.get('result', '')).strip()\n",
        "                if query:\n",
        "                    results[query] = json_result\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {filename} not found. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {filename}: {e}\")\n",
        "    return results\n",
        "\n",
        "def format_result_as_markdown(json_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a JSON result string into clean, human-readable Markdown.\n",
        "\n",
        "    Each vector collection becomes a section with a friendly heading,\n",
        "    the extracted subquery (or a clear \"not relevant\" note), and\n",
        "    a collapsed justification the reader can expand if they want detail.\n",
        "\n",
        "    Args:\n",
        "        json_str: Raw JSON string with per-collection results\n",
        "\n",
        "    Returns:\n",
        "        Markdown-formatted string ready for display\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cleaned = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', ' ', json_str)\n",
        "        data = json.loads(cleaned)\n",
        "    except (json.JSONDecodeError, TypeError) as e:\n",
        "        print(f\"Error parsing JSON: {e}\")\n",
        "        return json_str  # Return raw text when JSON is invalid\n",
        "\n",
        "    sections = []\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if value is None:\n",
        "            continue\n",
        "\n",
        "        # Use the friendly label, fall back to a cleaned-up key name\n",
        "        label = COLLECTION_LABELS.get(key, key.replace(\"_\", \" \").title())\n",
        "        subquery = value.get(\"relevant_subquery_text\")\n",
        "        justification = value.get(\"justification\", \"\")\n",
        "\n",
        "        # Build the section for this collection\n",
        "        section_lines = [f\"### {label}\"]\n",
        "\n",
        "        if subquery:\n",
        "            section_lines.append(f\"**Subquery:** {subquery}\")\n",
        "        else:\n",
        "            section_lines.append(\"*Not relevant to this query*\")\n",
        "\n",
        "        # Show justification in a collapsible details block\n",
        "        if justification:\n",
        "            section_lines.append(\"\")\n",
        "            section_lines.append(\"<details>\")\n",
        "            section_lines.append(\"<summary>Justification</summary>\")\n",
        "            section_lines.append(\"\")\n",
        "            section_lines.append(justification)\n",
        "            section_lines.append(\"</details>\")\n",
        "\n",
        "        sections.append(\"\\n\".join(section_lines))\n",
        "\n",
        "    print(f\"sections: {len(sections)}\")\n",
        "\n",
        "    return \"\\n\\n---\\n\\n\".join(sections)\n",
        "\n",
        "# Load results from both CSV files\n",
        "a_results = load_results_from_csv('../generated_data/ground_truth.csv')\n",
        "b_results = load_results_from_csv('../generated_data/claude_v3.csv')\n",
        "\n",
        "# Build a unified list combining results from both files\n",
        "all_queries = set(a_results.keys()) | set(b_results.keys())\n",
        "unified_results: List[Dict[str, str]] = []\n",
        "\n",
        "for query in sorted(all_queries):\n",
        "    unified_results.append({\n",
        "        \"query\": query,\n",
        "        \"a_result\": a_results.get(query, \"No result available\"),\n",
        "        \"b_result\": b_results.get(query, \"No result available\"),\n",
        "    })\n",
        "\n",
        "print(f\"Loaded {len(unified_results)} queries from both CSV files\")\n",
        "\n",
        "def display_results(selected_query: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Look up the selected query and return formatted Markdown for both results.\n",
        "\n",
        "    Args:\n",
        "        selected_query: The query string chosen from the dropdown\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (a_markdown, b_markdown) for the two display columns\n",
        "    \"\"\"\n",
        "    for result in unified_results:\n",
        "        if result[\"query\"] == selected_query:\n",
        "            return (\n",
        "                format_result_as_markdown(result[\"a_result\"]),\n",
        "                format_result_as_markdown(result[\"b_result\"]),\n",
        "            )\n",
        "    return \"Query not found\", \"Query not found\"\n",
        "\n",
        "# Dropdown choices\n",
        "query_options = [r[\"query\"] for r in unified_results]\n",
        "\n",
        "# Build the Gradio interface\n",
        "with gr.Blocks(title=\"Query Results Comparison\", theme=gr.themes.Soft()) as interface:\n",
        "    gr.Markdown(\"# Query Results Comparison\")\n",
        "    gr.Markdown(\"Select a query to compare results from **A** and **B**.\")\n",
        "\n",
        "    # Dropdown sits above the two result columns so it gets full width\n",
        "    query_dropdown = gr.Dropdown(\n",
        "        choices=query_options,\n",
        "        label=\"Select Query\",\n",
        "        value=query_options[0] if query_options else None,\n",
        "        interactive=True,\n",
        "    )\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        # Column A — rendered as Markdown for readable formatting\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Result A\")\n",
        "            a_output = gr.Markdown(value=\"\")\n",
        "\n",
        "        # Column B — rendered as Markdown for readable formatting\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## Result B\")\n",
        "            b_output = gr.Markdown(value=\"\")\n",
        "\n",
        "    # Wire up the dropdown to update both columns\n",
        "    query_dropdown.change(\n",
        "        fn=display_results,\n",
        "        inputs=query_dropdown,\n",
        "        outputs=[a_output, b_output],\n",
        "    )\n",
        "\n",
        "    # Show the first query's results on load\n",
        "    if query_options:\n",
        "        interface.load(\n",
        "            fn=lambda: display_results(query_options[0]),\n",
        "            outputs=[a_output, b_output],\n",
        "        )\n",
        "\n",
        "interface.launch(share=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54758bc",
      "metadata": {},
      "source": [
        "## Vector Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0dd61976",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RelevanceSize(Enum):\n",
        "    NOT_RELEVANT = \"not_relevant\"\n",
        "    SMALL = \"small\"\n",
        "    MEDIUM = \"medium\"\n",
        "    LARGE = \"large\"\n",
        "\n",
        "class VectorWeightResponse(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "\n",
        "    relevance: RelevanceSize\n",
        "    justification: str\n",
        "\n",
        "class VectorWeights(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "\n",
        "    plot_events_data: VectorWeightResponse\n",
        "    plot_analysis_data: VectorWeightResponse\n",
        "    viewer_experience_data: VectorWeightResponse\n",
        "    watch_context_data: VectorWeightResponse\n",
        "    narrative_techniques_data: VectorWeightResponse\n",
        "    production_data: VectorWeightResponse\n",
        "    reception_data: VectorWeightResponse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "7c4f5893",
      "metadata": {},
      "outputs": [],
      "source": [
        "# GENERATE VECTOR WEIGHTS\n",
        "\n",
        "def generate_single_vector_weight(system_prompt: str, query: str):\n",
        "    \"\"\"\n",
        "    Generic method for generating a single vector weight.\n",
        "    Takes a system prompt and query, runs generate_kimi_response with VectorWeightResponse format.\n",
        "    \n",
        "    Args:\n",
        "        system_prompt: The system prompt string for the LLM\n",
        "        query: The user query string\n",
        "        \n",
        "    Returns:\n",
        "        VectorWeightResponse from the LLM\n",
        "    \"\"\"\n",
        "    return generate_kimi_response(\n",
        "        user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "        system_prompt=system_prompt,\n",
        "        response_format=VectorWeightResponse\n",
        "    )\n",
        "\n",
        "def process_single_vector_weight_query(query: str) -> Tuple[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Process a single query by running the generic method for each prompt in VECTOR_WEIGHT_PROMPTS in parallel.\n",
        "    \n",
        "    Args:\n",
        "        query: The query string to process\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (query, results_dict) where results_dict maps prompt_name -> VectorWeightResponse (or error string)\n",
        "    \"\"\"\n",
        "    results_dict = {}\n",
        "    # Run all prompts for this query in parallel\n",
        "    with futures.ThreadPoolExecutor(max_workers=len(VECTOR_WEIGHT_PROMPTS)) as executor:\n",
        "        future_to_prompt = {\n",
        "            executor.submit(generate_single_vector_weight, prompt_str, query): prompt_name\n",
        "            for prompt_name, prompt_str in VECTOR_WEIGHT_PROMPTS.items()\n",
        "        }\n",
        "        for future in futures.as_completed(future_to_prompt):\n",
        "            prompt_name = future_to_prompt[future]\n",
        "            try:\n",
        "                results_dict[prompt_name] = future.result()\n",
        "            except Exception as e:\n",
        "                results_dict[prompt_name] = f\"Error: {str(e)}\"\n",
        "    return query, results_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24143090",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing queries: 100%|██████████| 31/31 [00:15<00:00,  2.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 31 queries and saved results to weights_results.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Process all queries in parallel (each query runs its prompts in parallel internally)\n",
        "results: List[Tuple[str, Dict[str, Any]]] = []\n",
        "\n",
        "with futures.ThreadPoolExecutor(max_workers=7) as executor:\n",
        "    # Submit process_single_query for each routing query\n",
        "    future_to_query = {executor.submit(process_single_vector_weight_query, query): query for query in routing_queries}\n",
        "    \n",
        "    # Collect results as they complete, with progress bar\n",
        "    for future in tqdm(futures.as_completed(future_to_query), total=len(routing_queries), desc=\"Processing queries\"):\n",
        "        query, results_dict = future.result()\n",
        "        results.append((query, results_dict))\n",
        "\n",
        "# Write results to CSV file\n",
        "with open('../generated_data/weights_results.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['query', 'result']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    \n",
        "    for query, results_dict in results:\n",
        "        # Serialize each result (VectorWeightResponse.model_dump() or keep error string)\n",
        "        serializable = {\n",
        "            k: (v.model_dump() if hasattr(v, 'model_dump') else v)\n",
        "            for k, v in results_dict.items()\n",
        "        }\n",
        "        writer.writerow({'query': query, 'result': json.dumps(serializable)})\n",
        "\n",
        "print(f\"Processed {len(results)} queries and saved results to weights_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065b88a3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7861\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "\n",
        "# Load the CSV file with query results\n",
        "csv_path = Path(\"../generated_data/weights_results.csv\")\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "def format_results_as_markdown(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Formats the query results as a nicely structured markdown string.\n",
        "    Handles the JSON format: {category: {relevance, justification}} or {category: \"Error: ...\"}.\n",
        "    \n",
        "    Args:\n",
        "        query: The selected query string\n",
        "        \n",
        "    Returns:\n",
        "        Formatted markdown string with query, relevance scores, and justifications\n",
        "    \"\"\"\n",
        "    # Find the row matching the selected query\n",
        "    row = df[df['query'] == query]\n",
        "    \n",
        "    if row.empty:\n",
        "        return \"Query not found.\"\n",
        "    \n",
        "    # Parse the result string (stored as JSON)\n",
        "    result_str = row.iloc[0]['result']\n",
        "    try:\n",
        "        result_dict = json.loads(result_str)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return f\"Error parsing results for query: {query}\"\n",
        "    \n",
        "    # Build the markdown output\n",
        "    markdown = f\"## Query\\n\\n**{query}**\\n\\n\"\n",
        "    markdown += \"## Relevance Scores\\n\\n\"\n",
        "    \n",
        "    # Group categories by relevance level; each value is {relevance, justification} or error string\n",
        "    relevance_levels = {\n",
        "        'large': [],\n",
        "        'medium': [],\n",
        "        'small': [],\n",
        "        'not_relevant': []\n",
        "    }\n",
        "    errors = []\n",
        "    \n",
        "    for category, value in result_dict.items():\n",
        "        category_display = category.replace('_', ' ').title()\n",
        "        if isinstance(value, str):\n",
        "            # Error string from failed API call\n",
        "            errors.append((category_display, value))\n",
        "        elif isinstance(value, dict) and 'relevance' in value:\n",
        "            relevance = value['relevance']\n",
        "            justification = value.get('justification', '')\n",
        "            if relevance in relevance_levels:\n",
        "                relevance_levels[relevance].append((category_display, justification))\n",
        "            else:\n",
        "                errors.append((category_display, f\"Unknown relevance: {relevance}\"))\n",
        "        else:\n",
        "            errors.append((category_display, \"Invalid result format\"))\n",
        "    \n",
        "    # Display by relevance level (high to low)\n",
        "    for level in ['large', 'medium', 'small', 'not_relevant']:\n",
        "        items = relevance_levels[level]\n",
        "        if items:\n",
        "            level_display = level.replace('_', ' ').title()\n",
        "            markdown += f\"### {level_display}\\n\\n\"\n",
        "            for category_display, justification in items:\n",
        "                markdown += f\"- **{category_display}**\\n\"\n",
        "                if justification:\n",
        "                    markdown += f\"  _{justification}_\\n\"\n",
        "            markdown += \"\\n\"\n",
        "    \n",
        "    # Show any errors at the end\n",
        "    if errors:\n",
        "        markdown += \"### Errors\\n\\n\"\n",
        "        for category_display, error_msg in errors:\n",
        "            markdown += f\"- **{category_display}**: {error_msg}\\n\"\n",
        "    \n",
        "    return markdown\n",
        "\n",
        "# Create the Gradio interface\n",
        "def create_interface():\n",
        "    \"\"\"\n",
        "    Creates and launches the Gradio interface for viewing query results.\n",
        "    \"\"\"\n",
        "    # Get list of queries for the dropdown\n",
        "    queries = df['query'].tolist()\n",
        "    \n",
        "    # Create the interface\n",
        "    interface = gr.Interface(\n",
        "        fn=format_results_as_markdown,\n",
        "        inputs=gr.Dropdown(\n",
        "            choices=queries,\n",
        "            label=\"Select Query\",\n",
        "            value=queries[0] if queries else None\n",
        "        ),\n",
        "        outputs=gr.Markdown(label=\"Results\"),\n",
        "        title=\"Query Understanding Results Viewer\",\n",
        "        description=\"Select a query to view its relevance scores for different vector categories.\"\n",
        "    )\n",
        "    \n",
        "    return interface\n",
        "\n",
        "# Launch the interface\n",
        "iface = create_interface()\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62403d0b",
      "metadata": {},
      "source": [
        "## Channel Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "b56325e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChannelWeights(BaseModel):\n",
        "    model_config = ConfigDict(use_enum_values=True)\n",
        "\n",
        "    lexical_relevance: RelevanceSize\n",
        "    metadata_relevance: RelevanceSize\n",
        "    vector_relevance: RelevanceSize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "37979efa",
      "metadata": {},
      "outputs": [],
      "source": [
        "CHANNEL_WEIGHTS_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are an expert at understanding search query intentions. You are an intent-to-weight router for a movie search system.\n",
        "\n",
        "TASK\n",
        "Given a single user search query, estimate the RELATIVE importance of three search channels:\n",
        "1) Lexical search (specific entities to find)\n",
        "2) Metadata preferences (concrete attributes to filter on. Only attributes from the list below.)\n",
        "3) Vector search (semantic intent to match)\n",
        "\n",
        "INPUT\n",
        "- The user's full movie search query (as typed into a search bar).\n",
        "\n",
        "OUTPUT (STRICT)\n",
        "JSON with these keys:\n",
        "- \"lexical_relevance\"\n",
        "- \"metadata_relevance\"\n",
        "- \"vector_relevance\"\n",
        "\n",
        "Value rules:\n",
        "- lexical_relevance / metadata_relevance / vector_relevance must be one of:\n",
        "  \"not_relevant\", \"small\", \"medium\", \"large\"\n",
        "\n",
        "RELEVANCY DEFINITIONS\n",
        "- \"not_relevant\": The query has absolutely no intent relevant to what this channel searches for.\n",
        "- \"small\": A small portion of the query's intent / search features are relevant to what this channel searches for.\n",
        "- \"medium\": A moderate portion of the query's intent / search features are relevant to what this channel searches for.\n",
        "- \"large\": Nearly all of the query's intent / search features are relevant to what this channel searches for.\n",
        "\n",
        "HOW TO THINK (HIGH LEVEL)\n",
        "- Each query is looking for one or more distinct features for their movie, with each one applying to one or more channels.\n",
        "- Overall a specific channel's relevance is what percentage of these distinct features are searched within this channel.\n",
        "\n",
        "CHANNEL DEFINITIONS\n",
        "\n",
        "A) Lexical search (lexical_relevance)\n",
        "The user is explicitly searching for one of the following:\n",
        "- character names\n",
        "- franchises / series names\n",
        "- real-world people (actors, directors, writers, composers, etc.)\n",
        "- real-world studios / production companies\n",
        "- movie titles\n",
        "\n",
        "Rules:\n",
        "- If the user likely misspelled a name/title but intent is clearly an entity, count it as lexical.\n",
        "- If a phrase could be either an entity (e.g., title) OR a descriptive phrase, count it as lexical AND also count it for whichever other channel(s) it fits.\n",
        "- There are no lexical entities beyond characters, franchises, people, studios, and movie titles. Do not make up new categories.\n",
        "\n",
        "B) Metadata preferences (metadata_relevance)\n",
        "ONLY the following attributes:\n",
        "- release date / decade / year\n",
        "- duration / runtime\n",
        "- genres\n",
        "- audio languages\n",
        "- streaming platforms\n",
        "- maturity rating\n",
        "- trending status (binary true or false)\n",
        "- popularity status (binary true or false)\n",
        "- reception level ONLY when explicitly framed as “good/bad” (e.g., “acclaimed”, “bad reviews”, “overrated”)\n",
        "\n",
        "Rules:\n",
        "- Some metadata attributes overlap with semantics (ex. genre). In that case count it towards both channels.\n",
        "- The attributes listed above are the only pieces of metadata we use. Only increase metadata_relevance if the query has parts that match these exact attributes.\n",
        "- Never add new metadata attributes beyond what I've listed above.\n",
        "\n",
        "C) Vector search (vector_relevance)\n",
        "Use this for semantic intent that is not purely a lexical entity or a structured metadata preference, including:\n",
        "- plot/story content (what happens, setting in-story, character motivations)\n",
        "- themes, arcs, generalized “what it's about”\n",
        "- viewer experience (tone, tension, intensity, disturbance, etc.)\n",
        "- watch context (why/when to watch, scenarios, motivations) if present in the query\n",
        "- storytelling techniques (unreliable narrator, nonlinear timeline, twist ending, etc.)\n",
        "- any ambiguous phrases that could plausibly be semantic descriptors\n",
        "\n",
        "Rules:\n",
        "- Vector search covers all movie attributes so it should always be included. It's weight increases the more the query asks for \"vibes\" or attributes that are hard to evaluate concretely (ex. \"Has jumpscares\")\n",
        "- Just because a part of the query applies to one channel doesn't mean it can't also apply to this one.\n",
        "\n",
        "CONSTRAINTS\n",
        "- Base your judgment ONLY on the raw query text.\n",
        "- My lists are gospel. If a part of the query doesn't match the description I've provided for a given channel, it's not relevant to this channel.\n",
        "- Do not assume access to any other extraction models or filters.\n",
        "- Do not output absolute numeric weights—only the allowed T-shirt sizes.\n",
        "- Double check: are you using metadata attributes not explicitly stated in my list above? If so, remove them.\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "9e3bf7f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_channel_weights_query(query: str):\n",
        "    \"\"\"Process a single query through generate_kimi_response for channel weights.\"\"\"\n",
        "    response = generate_kimi_response(\n",
        "        user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "        system_prompt=CHANNEL_WEIGHTS_SYSTEM_PROMPT,\n",
        "        response_format=ChannelWeights,\n",
        "    )\n",
        "    return (query, response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9a8fce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing queries: 100%|██████████| 31/31 [00:06<00:00,  5.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 31 channel weights responses\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'ChannelWeights' object has no attribute 'items'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m     writer.writeheader()\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m query, results_dict \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m     30\u001b[39m         \u001b[38;5;66;03m# Serialize each result (VectorWeightResponse.model_dump() or keep error string)\u001b[39;00m\n\u001b[32m     31\u001b[39m         serializable = {\n\u001b[32m     32\u001b[39m             k: (v.model_dump() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(v, \u001b[33m'\u001b[39m\u001b[33mmodel_dump\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m v)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresults_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m()\n\u001b[32m     34\u001b[39m         }\n\u001b[32m     35\u001b[39m         writer.writerow({\u001b[33m'\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m'\u001b[39m: query, \u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m: json.dumps(serializable)})\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSaved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path.resolve()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/movie-finder-rag/.venv/lib/python3.13/site-packages/pydantic/main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: 'ChannelWeights' object has no attribute 'items'"
          ]
        }
      ],
      "source": [
        "results: List[Tuple[str, Dict[str, Any]]] = []\n",
        "\n",
        "with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    # Submit process_single_query for each routing query\n",
        "    future_to_query = {executor.submit(process_channel_weights_query, query): query for query in routing_queries}\n",
        "    \n",
        "    # Collect results as they complete, with progress bar\n",
        "    for future in tqdm(futures.as_completed(future_to_query), total=len(routing_queries), desc=\"Processing queries\"):\n",
        "        query, results_dict = future.result()\n",
        "        results.append((query, results_dict))\n",
        "\n",
        "print(f\"Generated {len(results)} channel weights responses\")\n",
        "\n",
        "# Write results to CSV: query plus each response's relevance fields\n",
        "with open('../generated_data/channel_weights.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    fieldnames = ['query', 'result']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    \n",
        "    # Write header row\n",
        "    writer.writeheader()\n",
        "    \n",
        "    # Write data rows\n",
        "    for query, results_dict in results:\n",
        "        row = {\n",
        "            'query': query,\n",
        "            'result': results_dict.model_dump_json()\n",
        "        }\n",
        "        \n",
        "        writer.writerow(row)\n",
        "    \n",
        "print(f\"Saved to {csv_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43318b6a",
      "metadata": {},
      "source": [
        "# Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b333abc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing routing queries: 100%|██████████| 31/31 [03:39<00:00,  7.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 31 results to /Users/michaelkeohane/Documents/movie-finder-rag/implementation/notebooks/overall_results.csv\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_channel_weights(query):\n",
        "    \"\"\"Extract channel weights (lexical, metadata, vector relevance) from query.\"\"\"\n",
        "    return process_channel_weights_query(query)\n",
        "\n",
        "def get_lexical_entities(query):\n",
        "    \"\"\"Extract lexical entities (characters, franchises, people, studios, titles) from query.\"\"\"\n",
        "    return generate_openai_response(\n",
        "        user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "        system_prompt=EXTRACT_LEXICAL_ENTITIES_SYSTEM_PROMPT,\n",
        "        response_format=ExtractedEntities,\n",
        "    )\n",
        "\n",
        "def get_metadata_preferences(query):\n",
        "    \"\"\"Extract metadata preferences (date, duration, genres, etc.) from query.\"\"\"\n",
        "    return generate_kimi_response(\n",
        "        user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "        system_prompt=EXTRACT_METADATA_PREFERENCES_SYSTEM_PROMPT,\n",
        "        response_format=MetadataPreferences,\n",
        "    )\n",
        "\n",
        "def _process_single_vector_query(query: str, system_prompt: str, key: str) -> tuple[str, VectorCollectionQueryData]:\n",
        "    \"\"\"\n",
        "    Process a single vector routing query.\n",
        "    \n",
        "    Args:\n",
        "        query: The user's search query\n",
        "        system_prompt: The system prompt for this vector collection\n",
        "        key: The key name for this vector collection\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (key, VectorCollectionQueryData response)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = generate_kimi_response(\n",
        "            user_prompt=f\"User query: \\\"{query}\\\"\",\n",
        "            system_prompt=system_prompt,\n",
        "            response_format=VectorCollectionQueryData\n",
        "        )\n",
        "        return key, response\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {key}: {e}\")\n",
        "        return key, None\n",
        "\n",
        "def _process_single_vector_weight(prompt_name: str, prompt_str: str, query: str) -> tuple[str, Any]:\n",
        "    \"\"\"\n",
        "    Process a single vector weight query.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (prompt_name, VectorWeightResponse or error string)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = generate_single_vector_weight(prompt_str, query)\n",
        "        return prompt_name, response\n",
        "    except Exception as e:\n",
        "        return prompt_name, f\"Error: {str(e)}\"\n",
        "\n",
        "def get_query_understanding(query: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extract all query understanding components in parallel.\n",
        "    \n",
        "    Runs 17 LLM calls in a single flat executor (no nested parallelization):\n",
        "    - 1 lexical entity extraction\n",
        "    - 1 metadata preferences\n",
        "    - 1 channel weights\n",
        "    - 7 vector subqueries (VECTOR_QUERY_PROMPTS style)\n",
        "    - 7 vector weights (VECTOR_WEIGHT_PROMPTS)\n",
        "    \n",
        "    Args:\n",
        "        query: The user's movie search query\n",
        "        \n",
        "    Returns:\n",
        "        Dict with channel_weights, lexical_entities, metadata_preferences,\n",
        "        vector_routing, vector_weights\n",
        "    \"\"\"\n",
        "    # Vector subqueries: same structure as get_vector_queries (VECTOR_QUERY_PROMPTS)\n",
        "    vector_subquery_tasks = [\n",
        "        (VECTOR_QUERY_PROMPTS[\"plot_events\"], \"plot_events_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"plot_analysis\"], \"plot_analysis_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"viewer_experience\"], \"viewer_experience_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"watch_context\"], \"watch_context_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"narrative_techniques\"], \"narrative_techniques_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"production\"], \"production_data\"),\n",
        "        (VECTOR_QUERY_PROMPTS[\"reception\"], \"reception_data\"),\n",
        "    ]\n",
        "    \n",
        "    # Vector weight tasks: one per prompt in VECTOR_WEIGHT_PROMPTS\n",
        "    vector_weight_tasks = [\n",
        "        (prompt_name, prompt_str)\n",
        "        for prompt_name, prompt_str in VECTOR_WEIGHT_PROMPTS.items()\n",
        "    ]\n",
        "    \n",
        "    with futures.ThreadPoolExecutor(max_workers=17) as executor:\n",
        "        # Submit all 17 tasks\n",
        "        future_channel = executor.submit(get_channel_weights, query)\n",
        "        future_lexical = executor.submit(get_lexical_entities, query)\n",
        "        future_metadata = executor.submit(get_metadata_preferences, query)\n",
        "        \n",
        "        vector_subquery_futures = {\n",
        "            executor.submit(_process_single_vector_query, query, system_prompt, key): key\n",
        "            for system_prompt, key in vector_subquery_tasks\n",
        "        }\n",
        "        \n",
        "        vector_weight_futures = {\n",
        "            executor.submit(_process_single_vector_weight, prompt_name, prompt_str, query): prompt_name\n",
        "            for prompt_name, prompt_str in vector_weight_tasks\n",
        "        }\n",
        "        \n",
        "        # Collect vector subquery results\n",
        "        vector_results = {}\n",
        "        for future in futures.as_completed(vector_subquery_futures):\n",
        "            key, response = future.result()\n",
        "            if response:\n",
        "                vector_results[key] = response\n",
        "        \n",
        "        # Collect vector weight results\n",
        "        vector_weight_results = {}\n",
        "        for future in futures.as_completed(vector_weight_futures):\n",
        "            prompt_name, response = future.result()\n",
        "            vector_weight_results[prompt_name] = response\n",
        "        \n",
        "        vector_routing = VectorRoutingResponse(\n",
        "            plot_events_data=vector_results.get(\"plot_events_data\"),\n",
        "            plot_analysis_data=vector_results.get(\"plot_analysis_data\"),\n",
        "            viewer_experience_data=vector_results.get(\"viewer_experience_data\"),\n",
        "            watch_context_data=vector_results.get(\"watch_context_data\"),\n",
        "            narrative_techniques_data=vector_results.get(\"narrative_techniques_data\"),\n",
        "            production_data=vector_results.get(\"production_data\"),\n",
        "            reception_data=vector_results.get(\"reception_data\")\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"channel_weights\": future_channel.result(),\n",
        "            \"lexical_entities\": future_lexical.result(),\n",
        "            \"metadata_preferences\": future_metadata.result(),\n",
        "            \"vector_routing\": vector_routing,\n",
        "            \"vector_weights\": vector_weight_results,\n",
        "        }\n",
        "\n",
        "def _serialize_for_json(obj: Any) -> Any:\n",
        "    \"\"\"Convert Pydantic models and nested structures to JSON-serializable dicts.\"\"\"\n",
        "    if hasattr(obj, \"model_dump\"):\n",
        "        return obj.model_dump(mode=\"json\")\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: _serialize_for_json(v) for k, v in obj.items()}\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [_serialize_for_json(v) for v in obj]\n",
        "    return obj\n",
        "\n",
        "# Process each routing query and save to overall_results.csv\n",
        "overall_results = []\n",
        "for query in tqdm(routing_queries, desc=\"Processing routing queries\"):\n",
        "    result = get_query_understanding(query)\n",
        "    # Build JSON-serializable dict of all fetched attributes\n",
        "    serialized = {\n",
        "        \"channel_weights\": _serialize_for_json(result[\"channel_weights\"]),\n",
        "        \"lexical_entities\": _serialize_for_json(result[\"lexical_entities\"]),\n",
        "        \"metadata_preferences\": _serialize_for_json(result[\"metadata_preferences\"]),\n",
        "        \"vector_routing\": _serialize_for_json(result[\"vector_routing\"]),\n",
        "        \"vector_weights\": {\n",
        "            k: _serialize_for_json(v) if hasattr(v, \"model_dump\") else v\n",
        "            for k, v in result[\"vector_weights\"].items()\n",
        "        },\n",
        "    }\n",
        "    overall_results.append({\"query\": query, \"results\": json.dumps(serialized)})\n",
        "\n",
        "csv_path = Path(\"../generated_data/overall_results.csv\")\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"query\", \"results\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(overall_results)\n",
        "\n",
        "print(f\"Saved {len(overall_results)} results to {csv_path.resolve()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6676a992",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7864\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gradio interface for overall_results.csv\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def _format_value(val, indent=0):\n",
        "    \"\"\"Recursively format a value for display (handles dicts, lists, None).\"\"\"\n",
        "    if val is None:\n",
        "        return \"_none_\"\n",
        "    if isinstance(val, bool):\n",
        "        return str(val)\n",
        "    if isinstance(val, (int, float)):\n",
        "        return str(val)\n",
        "    if isinstance(val, str):\n",
        "        return val\n",
        "    if isinstance(val, list):\n",
        "        if not val:\n",
        "            return \"[]\"\n",
        "        items = [_format_value(v, indent + 1) for v in val]\n",
        "        return \"\\n\" + \"  \" * (indent + 1) + (\"\\n\" + \"  \" * (indent + 1)).join(f\"- {x}\" for x in items)\n",
        "    if isinstance(val, dict):\n",
        "        lines = []\n",
        "        for k, v in val.items():\n",
        "            if v is None or v == \"\" or v == [] or v == {}:\n",
        "                continue\n",
        "            lines.append(f\"**{k.replace('_', ' ').title()}:** {_format_value(v, indent + 1)}\")\n",
        "        return \"\\n\" + \"  \" * (indent + 1) + (\"\\n\" + \"  \" * (indent + 1)).join(lines)\n",
        "    return str(val)\n",
        "\n",
        "\n",
        "def _format_metadata_preferences(mp: dict) -> str:\n",
        "    \"\"\"\n",
        "    Format metadata preferences as a bulleted list grouped by high-level keys.\n",
        "    Each top-level key (e.g., release_date_preference) becomes a bullet group\n",
        "    with its sub-items as nested bullets.\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for key, val in mp.items():\n",
        "        label = key.replace(\"_\", \" \").title()\n",
        "        if val is None:\n",
        "            continue\n",
        "        if isinstance(val, dict):\n",
        "            sub_items = []\n",
        "            for k, v in val.items():\n",
        "                sub_label = k.replace(\"_\", \" \").title()\n",
        "                if isinstance(v, list):\n",
        "                    sub_val = \", \".join(str(x) for x in v) if v else \"(none)\"\n",
        "                    sub_items.append(f\"- **{sub_label}:** {sub_val}\")\n",
        "                elif v is None:\n",
        "                    sub_items.append(f\"- **{sub_label}:** (none)\")\n",
        "                else:\n",
        "                    sub_items.append(f\"- **{sub_label}:** {v}\")\n",
        "            if sub_items:\n",
        "                lines.append(f\"- **{label}**\")\n",
        "                lines.extend(f\"  {s}\" for s in sub_items)\n",
        "        elif isinstance(val, list):\n",
        "            lines.append(f\"- **{label}:** {', '.join(str(x) for x in val) if val else '(none)'}\")\n",
        "        else:\n",
        "            lines.append(f\"- **{label}:** {val}\")\n",
        "    return \"\\n\".join(lines) if lines else \"_No metadata preferences._\"\n",
        "\n",
        "\n",
        "def _escape_html(text: str) -> str:\n",
        "    \"\"\"Escape HTML special chars so content doesn't break tags.\"\"\"\n",
        "    return text.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace('\"', \"&quot;\")\n",
        "\n",
        "def _collapsible_section(title: str, content: str, open_by_default: bool = False) -> str:\n",
        "    \"\"\"Build HTML collapsible section for justifications.\"\"\"\n",
        "    open_attr = \" open\" if open_by_default else \"\"\n",
        "    safe_content = _escape_html(content)\n",
        "    return f'''\n",
        "<details{open_attr}>\n",
        "<summary><strong>{title}</strong></summary>\n",
        "<p style=\"margin: 0.5em 0; line-height: 1.5;\">{safe_content}</p>\n",
        "</details>\n",
        "'''\n",
        "\n",
        "\n",
        "def format_overall_results(query: str, df: pd.DataFrame) -> str:\n",
        "    \"\"\"\n",
        "    Format the query results from overall_results.csv for display.\n",
        "    Justifications are placed in collapsible <details> sections.\n",
        "    \n",
        "    Args:\n",
        "        query: The selected query string\n",
        "        df: DataFrame loaded from overall_results.csv\n",
        "        \n",
        "    Returns:\n",
        "        HTML/Markdown string with formatted results\n",
        "    \"\"\"\n",
        "    row = df[df[\"query\"] == query]\n",
        "    if row.empty:\n",
        "        return \"Query not found.\"\n",
        "    \n",
        "    result_str = row.iloc[0][\"results\"]\n",
        "    try:\n",
        "        data = json.loads(result_str)\n",
        "    except (json.JSONDecodeError, TypeError):\n",
        "        return f\"Error parsing results for query: {query}\"\n",
        "    \n",
        "    parts = [f\"# Query\\n\\n> **{query}**\\n\"]\n",
        "    \n",
        "    # Channel weights\n",
        "    cw = data.get(\"channel_weights\")\n",
        "    if cw is not None:\n",
        "        # Handle format: [query, dict] or just dict\n",
        "        weights = cw[1] if isinstance(cw, list) and len(cw) > 1 else (cw if isinstance(cw, dict) else {})\n",
        "        if weights:\n",
        "            parts.append(\"## Channel Weights\\n\")\n",
        "            parts.append(f\"- **Lexical relevance:** {weights.get('lexical_relevance', 'N/A')}\")\n",
        "            parts.append(f\"- **Metadata relevance:** {weights.get('metadata_relevance', 'N/A')}\")\n",
        "            parts.append(f\"- **Vector relevance:** {weights.get('vector_relevance', 'N/A')}\\n\")\n",
        "    \n",
        "    # Lexical entities\n",
        "    le = data.get(\"lexical_entities\", {})\n",
        "    entity_candidates = le.get(\"entity_candidates\", [])\n",
        "    parts.append(\"## Lexical Entities\\n\")\n",
        "    if entity_candidates:\n",
        "        for e in entity_candidates:\n",
        "            parts.append(f\"- **{e.get('corrected_and_normalized_entity', 'N/A')}** ({e.get('most_likely_category', '')})\")\n",
        "    else:\n",
        "        parts.append(\"_No entities extracted._\\n\")\n",
        "    \n",
        "    # Metadata preferences (bulleted list grouped by high-level keys)\n",
        "    mp = data.get(\"metadata_preferences\", {})\n",
        "    if mp:\n",
        "        parts.append(\"## Metadata Preferences\\n\")\n",
        "        parts.append(_format_metadata_preferences(mp) + \"\\n\")\n",
        "    \n",
        "    # Vectors: combined weight + subquery + justifications per collection\n",
        "    vr = data.get(\"vector_routing\", {}) or {}\n",
        "    vw = data.get(\"vector_weights\", {}) or {}\n",
        "    vector_collections = [\n",
        "        (\"plot_events_data\", \"plot_events\", \"Plot Events\"),\n",
        "        (\"plot_analysis_data\", \"plot_analysis\", \"Plot Analysis\"),\n",
        "        (\"viewer_experience_data\", \"viewer_experience\", \"Viewer Experience\"),\n",
        "        (\"watch_context_data\", \"watch_context\", \"Watch Context\"),\n",
        "        (\"narrative_techniques_data\", \"narrative_techniques\", \"Narrative Techniques\"),\n",
        "        (\"production_data\", \"production\", \"Production\"),\n",
        "        (\"reception_data\", \"reception\", \"Reception\"),\n",
        "    ]\n",
        "    if vr or vw:\n",
        "        parts.append(\"## Vectors\\n\")\n",
        "        for routing_key, weight_key, label in vector_collections:\n",
        "            routing_val = vr.get(routing_key)\n",
        "            weight_val = vw.get(weight_key)\n",
        "            if routing_val is None and weight_val is None:\n",
        "                continue\n",
        "            parts.append(f\"### {label}\\n\")\n",
        "            # Weight (relevance)\n",
        "            if isinstance(weight_val, str):\n",
        "                parts.append(f\"**Weight:** _Error: {weight_val}_\\n\")\n",
        "            elif isinstance(weight_val, dict):\n",
        "                relevance = weight_val.get(\"relevance\", \"N/A\")\n",
        "                weight_just = weight_val.get(\"justification\", \"\")\n",
        "                parts.append(f\"**Weight:** {relevance}\\n\")\n",
        "                if weight_just:\n",
        "                    parts.append(_collapsible_section(\"Weight justification\", weight_just))\n",
        "            else:\n",
        "                parts.append(\"**Weight:** N/A\\n\")\n",
        "            # Subquery\n",
        "            if isinstance(routing_val, dict):\n",
        "                subquery = routing_val.get(\"relevant_subquery_text\") or \"_none_\"\n",
        "                subquery_just = routing_val.get(\"justification\", \"\")\n",
        "                parts.append(f\"**Subquery:** `{subquery}`\\n\")\n",
        "                if subquery_just:\n",
        "                    parts.append(_collapsible_section(\"Subquery justification\", subquery_just))\n",
        "            elif routing_val is None and isinstance(weight_val, dict):\n",
        "                parts.append(\"**Subquery:** _none_\\n\")\n",
        "            parts.append(\"\\n\")\n",
        "    \n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "# Load overall_results.csv\n",
        "csv_path = Path(\"../generated_data/overall_results.csv\")\n",
        "if not csv_path.exists():\n",
        "    raise FileNotFoundError(f\"overall_results.csv not found at {csv_path.resolve()}\")\n",
        "\n",
        "df_overall = pd.read_csv(csv_path)\n",
        "query_choices = df_overall[\"query\"].tolist()\n",
        "\n",
        "# Build Gradio interface\n",
        "with gr.Blocks(title=\"Query Understanding Results\", theme=gr.themes.Soft()) as overall_interface:\n",
        "    gr.Markdown(\"# Query Understanding Results Viewer\")\n",
        "    gr.Markdown(\"Select a query to view its full extraction results. Justifications are in collapsible sections.\")\n",
        "    \n",
        "    query_dropdown = gr.Dropdown(\n",
        "        choices=query_choices,\n",
        "        value=query_choices[0] if query_choices else None,\n",
        "        label=\"Select Query\",\n",
        "        allow_custom_value=False,\n",
        "    )\n",
        "    \n",
        "    results_output = gr.Markdown(\n",
        "        value=format_overall_results(query_choices[0], df_overall) if query_choices else \"No data loaded.\"\n",
        "    )\n",
        "    \n",
        "    def on_query_change(query):\n",
        "        if not query:\n",
        "            return \"Select a query.\"\n",
        "        return format_overall_results(query, df_overall)\n",
        "    \n",
        "    query_dropdown.change(fn=on_query_change, inputs=[query_dropdown], outputs=[results_output])\n",
        "\n",
        "# Launch (use share=False for local only)\n",
        "overall_interface.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
