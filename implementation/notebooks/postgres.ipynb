{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "166accc8",
      "metadata": {},
      "source": [
        "# Postgres Methods & Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "db49ae82",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "# import redis\n",
        "\n",
        "# from qdrant_client import QdrantClient\n",
        "from pathlib import Path\n",
        "from typing import Optional, Sequence\n",
        "\n",
        "# Add parent directory to path to import from implementation package\n",
        "# Notebooks are in implementation/notebooks/, so we go up two levels to project root\n",
        "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
        "\n",
        "from implementation.classes.movie import BaseMovie\n",
        "from db.ingest_movie import ingest_movie\n",
        "from db.postgres import pool, refresh_title_token_doc_frequency\n",
        "\n",
        "# # Redis\n",
        "# r = redis.Redis(host=\"localhost\", port=6379, decode_responses=True)\n",
        "\n",
        "# # Qdrant\n",
        "# qdrant = QdrantClient(host=\"localhost\", port=6333)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "184545f7",
      "metadata": {},
      "source": [
        "## SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0406290c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the pool and establish initial connections\n",
        "await pool.open()\n",
        "# Validate that connections actually work (fast-fail if Postgres is unreachable)\n",
        "await pool.check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3d27822",
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Gracefully close all connections on shutdown\n",
        "# await pool.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e04ba2ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOAD MOVIES\n",
        "\n",
        "json_path = Path(\"../../saved_imdb_movies.json\")\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    movies_data = json.load(f)\n",
        "\n",
        "# Convert each dictionary to an IMDBMovie object\n",
        "movies = [BaseMovie(**movie_dict) for movie_dict in movies_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f426a136",
      "metadata": {},
      "source": [
        "## INGESTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "88b62cde",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingesting movies:   0%|          | 0/50 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: unrated and rank: 999.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: unrated and rank: 999.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: g and rank: 1.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: r and rank: 4.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n",
            "Maturity rating: pg-13 and rank: 3.\n",
            "Maturity rating: pg and rank: 2.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ingesting movies: 100%|██████████| 50/50 [00:03<00:00, 13.87it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None,\n",
              " None]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm.asyncio import tqdm\n",
        "\n",
        "# Run ingest_movie on every movie in parallel; tqdm.gather shows progress without blocking async\n",
        "await tqdm.gather(*[ingest_movie(movie) for movie in movies], desc=\"Ingesting movies\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5d471acb",
      "metadata": {},
      "outputs": [],
      "source": [
        "await refresh_title_token_doc_frequency()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d22f13a3",
      "metadata": {},
      "source": [
        "## SEARCHING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "919a7ba7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import re\n",
        "from dataclasses import dataclass, fields\n",
        "from implementation.misc.helpers import normalize_string\n",
        "\n",
        "# ─── Operational constants (from lexical search guide Section 11) ─────────────\n",
        "MAX_DF = 10_000\n",
        "TITLE_SCORE_BETA = 2.0\n",
        "TITLE_SCORE_THRESHOLD = 0.15\n",
        "TITLE_MAX_CANDIDATES = 10_000\n",
        "# Maximum term_ids returned per query phrase during resolution.\n",
        "# Phrases matching more character names than this are too vague to be\n",
        "# useful and would bloat the posting join.\n",
        "CHARACTER_RESOLVE_LIMIT_PER_PHRASE: int = 500\n",
        "\n",
        "# Fuzzy tier boundaries (Elasticsearch AUTO standard)\n",
        "EXACT_ONLY_MAX_LEN = 2    # Tokens this short get exact match only\n",
        "FUZZY_LEV1_MAX_LEN = 5    # Tokens 3–5 chars get lev ≤ 1\n",
        "                           # Tokens 6+ chars get lev ≤ 2\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LexicalCandidate:\n",
        "    \"\"\"Per-movie scoring components returned by the full lexical search.\"\"\"\n",
        "    movie_id: int\n",
        "    matched_people_count: int = 0\n",
        "    matched_character_count: int = 0\n",
        "    matched_studio_count: int = 0\n",
        "    title_score_sum: float = 0.0\n",
        "    raw_lexical_score: float = 0.0\n",
        "\n",
        "@dataclass(frozen=True, slots=True)\n",
        "class MetadataFilters:\n",
        "    \"\"\"\n",
        "    Metadata hard-filter parameters passed through from the query\n",
        "    understanding layer.  Any field left as None is inactive.\n",
        "\n",
        "    This dataclass is shared across all posting-search functions so\n",
        "    each can build its own MATERIALIZED eligible CTE without receiving\n",
        "    a pre-resolved ID array.\n",
        "    \"\"\"\n",
        "\n",
        "    min_release_ts: Optional[int] = None\n",
        "    max_release_ts: Optional[int] = None\n",
        "    min_runtime: Optional[int] = None\n",
        "    max_runtime: Optional[int] = None\n",
        "    max_maturity_rank: Optional[int] = None\n",
        "    genre_ids: Optional[list[int]] = None\n",
        "    watch_offer_keys: Optional[list[int]] = None\n",
        "\n",
        "    @property\n",
        "    def is_active(self) -> bool:\n",
        "        \"\"\"True when at least one filter is set.\"\"\"\n",
        "        return any(getattr(self, f.name) is not None for f in fields(self))\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  0. Misc Helpers\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# Pre-compiled regex for escaping LIKE pattern metacharacters.\n",
        "_LIKE_ESCAPE_RE = re.compile(r\"([\\\\%_])\")\n",
        "\n",
        "\n",
        "def _escape_like(value: str) -> str:\n",
        "    r\"\"\"\n",
        "    Escape SQL LIKE metacharacters so *value* is treated as a literal\n",
        "    substring.  Uses ``\\`` as the escape character.\n",
        "\n",
        "    >>> _escape_like(\"100%_done\")\n",
        "    '100\\\\%\\\\_done'\n",
        "    \"\"\"\n",
        "    return _LIKE_ESCAPE_RE.sub(r\"\\\\\\1\", value)\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  1. Term Resolution — Dictionary Lookups (Lexical Guide §7)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "\n",
        "async def resolve_phrase_term_ids(phrases: list[str]) -> dict[str, int]:\n",
        "    \"\"\"\n",
        "    Batch exact lookup of normalized phrases in lex.lexical_dictionary.\n",
        "\n",
        "    Used for people, character, and studio entity resolution at query time.\n",
        "    Phrases not present in the dictionary are silently omitted (they produce\n",
        "    no candidates).\n",
        "\n",
        "    Args:\n",
        "        phrases: List of already-normalized phrase strings.\n",
        "\n",
        "    Returns:\n",
        "        Mapping of norm_str → string_id for every phrase that exists.\n",
        "    \"\"\"\n",
        "    if not phrases:\n",
        "        return {}\n",
        "\n",
        "    query = \"\"\"\n",
        "        SELECT norm_str, string_id\n",
        "        FROM lex.lexical_dictionary\n",
        "        WHERE norm_str = ANY(%s::text[])\n",
        "    \"\"\"\n",
        "    async with pool.connection() as conn:\n",
        "        async with conn.cursor() as cur:\n",
        "            await cur.execute(query, (phrases,))\n",
        "            rows = await cur.fetchall()\n",
        "\n",
        "    return {row[0]: row[1] for row in rows}\n",
        "\n",
        "\n",
        "async def resolve_title_token_ids(token: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    Resolve a single normalized title token to candidate string_ids using\n",
        "    a tiered fuzzy strategy based on token length (Elasticsearch AUTO model).\n",
        "\n",
        "    Tiers:\n",
        "      - len ≤ 2:  Exact match only. Short tokens are too ambiguous for\n",
        "                   fuzzy matching (lev=1 on a 2-char token changes 50%).\n",
        "      - len 3-5:  Trigram shortlist + Levenshtein ≤ 1. Catches single-key\n",
        "                   typos while keeping results tight.\n",
        "      - len ≥ 6:  Trigram shortlist + Levenshtein ≤ 2. Longer words warrant\n",
        "                   more tolerance — lev=2 on a 10-char word is only 20%.\n",
        "\n",
        "    All tiers enforce MAX_DF filtering via lex.title_token_doc_frequency.\n",
        "    Exact matches are always prioritised in ordering.\n",
        "\n",
        "    Args:\n",
        "        token: A single normalized query token.\n",
        "\n",
        "    Returns:\n",
        "        List of candidate string_ids matching the token.\n",
        "    \"\"\"\n",
        "    token_len = len(token)\n",
        "\n",
        "    if token_len <= EXACT_ONLY_MAX_LEN:\n",
        "        # Tier 1: exact match only — skip trigram scan entirely\n",
        "        query = \"\"\"\n",
        "            SELECT d.string_id\n",
        "            FROM lex.title_token_strings d\n",
        "            JOIN lex.title_token_doc_frequency df\n",
        "              ON df.term_id = d.string_id\n",
        "            WHERE d.norm_str = %s\n",
        "              AND df.doc_frequency <= %s\n",
        "        \"\"\"\n",
        "        params = (token, MAX_DF)\n",
        "\n",
        "    elif token_len <= FUZZY_LEV1_MAX_LEN:\n",
        "        # Tier 2 (len 3–5): trigram shortlist + lev ≤ 1\n",
        "        query = \"\"\"\n",
        "            SELECT d.string_id\n",
        "            FROM lex.title_token_strings d\n",
        "            JOIN lex.title_token_doc_frequency df\n",
        "              ON df.term_id = d.string_id\n",
        "            WHERE\n",
        "              abs(length(d.norm_str) - length(%s)) <= 1\n",
        "              AND levenshtein(d.norm_str, %s) <= 1\n",
        "              AND df.doc_frequency <= %s\n",
        "            ORDER BY\n",
        "              (d.norm_str = %s) DESC,\n",
        "              similarity(d.norm_str, %s) DESC\n",
        "            LIMIT 20\n",
        "        \"\"\"\n",
        "        params = (token, token, MAX_DF, token, token)\n",
        "\n",
        "    else:\n",
        "        # Tier 3 (len ≥ 6): trigram shortlist + lev ≤ 2\n",
        "        query = \"\"\"\n",
        "            SELECT d.string_id\n",
        "            FROM lex.title_token_strings d\n",
        "            JOIN lex.title_token_doc_frequency df\n",
        "              ON df.term_id = d.string_id\n",
        "            WHERE\n",
        "              (d.norm_str = %s OR d.norm_str %% %s)\n",
        "              AND abs(length(d.norm_str) - length(%s)) <= 2\n",
        "              AND levenshtein(d.norm_str, %s) <= 2\n",
        "              AND df.doc_frequency <= %s\n",
        "            ORDER BY\n",
        "              (d.norm_str = %s) DESC,\n",
        "              similarity(d.norm_str, %s) DESC\n",
        "            LIMIT 30\n",
        "        \"\"\"\n",
        "        params = (token, token, token, token, MAX_DF, token, token)\n",
        "\n",
        "    async with pool.connection() as conn:\n",
        "        async with conn.cursor() as cur:\n",
        "            await cur.execute(query, params)\n",
        "            rows = await cur.fetchall()\n",
        "\n",
        "    return [row[0] for row in rows]\n",
        "\n",
        "\n",
        "async def resolve_character_term_ids(\n",
        "    character_phrases: list[str],\n",
        ") -> dict[int, list[int]]:\n",
        "    \"\"\"\n",
        "    Resolve normalized character query phrases to term_ids via substring\n",
        "    matching against lex.character_strings.\n",
        "\n",
        "    Batches all phrases into a single round-trip.  Each phrase is matched\n",
        "    with LIKE '%phrase%' accelerated by the trigram GIN index (for\n",
        "    phrases >= 3 chars; shorter ones fall back to a seqscan of the\n",
        "    small character_strings table).\n",
        "\n",
        "    Results are capped at CHARACTER_RESOLVE_LIMIT_PER_PHRASE per phrase.\n",
        "\n",
        "    Args:\n",
        "        character_phrases: Normalized character phrase strings.\n",
        "\n",
        "    Returns:\n",
        "        Dict of {query_phrase_idx: [term_id, ...]} where the index\n",
        "        corresponds to the position in the input list.\n",
        "    \"\"\"\n",
        "    if not character_phrases:\n",
        "        return {}\n",
        "\n",
        "    query_idxs: list[int] = []\n",
        "    like_patterns: list[str] = []\n",
        "    for idx, phrase in enumerate(character_phrases):\n",
        "        query_idxs.append(idx)\n",
        "        like_patterns.append(f\"%{_escape_like(phrase)}%\")\n",
        "\n",
        "    query = r\"\"\"\n",
        "        SELECT sub.query_idx, sub.string_id\n",
        "        FROM (\n",
        "            SELECT\n",
        "                qc.query_idx,\n",
        "                cs.string_id,\n",
        "                ROW_NUMBER() OVER (\n",
        "                    PARTITION BY qc.query_idx\n",
        "                    ORDER BY length(cs.norm_str)\n",
        "                ) AS rn\n",
        "            FROM unnest(%s::int[], %s::text[]) AS qc(query_idx, like_pattern)\n",
        "            JOIN lex.character_strings cs\n",
        "              ON cs.norm_str LIKE qc.like_pattern ESCAPE '\\'\n",
        "        ) sub\n",
        "        WHERE sub.rn <= %s\n",
        "    \"\"\"\n",
        "    params = [query_idxs, like_patterns, CHARACTER_RESOLVE_LIMIT_PER_PHRASE]\n",
        "\n",
        "    async with pool.connection() as conn:\n",
        "        async with conn.cursor() as cur:\n",
        "            await cur.execute(query, params)\n",
        "            rows = await cur.fetchall()\n",
        "\n",
        "    result: dict[int, list[int]] = {}\n",
        "    for query_idx, string_id in rows:\n",
        "        result.setdefault(query_idx, []).append(string_id)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "async def _resolve_all_title_tokens(\n",
        "    include_title_searches: list[list[str]],\n",
        ") -> list[dict[int, list[int]]]:\n",
        "    \"\"\"\n",
        "    Resolve all title tokens across all title searches concurrently.\n",
        "\n",
        "    Deduplicates tokens so each unique token is resolved once, then\n",
        "    distributes results back into per-title-search maps.\n",
        "\n",
        "    Args:\n",
        "        include_title_searches: List of token lists (each is one title search).\n",
        "\n",
        "    Returns:\n",
        "        List of token_map dicts, one per title search.\n",
        "    \"\"\"\n",
        "    print(include_title_searches)\n",
        "    # Collect (search_idx, token_idx, token) for every token\n",
        "    triples: list[tuple[int, int, str]] = []\n",
        "    for search_idx, title_tokens in enumerate(include_title_searches):\n",
        "        for token_idx, token in enumerate(title_tokens):\n",
        "            triples.append((search_idx, token_idx, token))\n",
        "\n",
        "    if not triples:\n",
        "        return []\n",
        "\n",
        "    # Deduplicate tokens while preserving first occurrence for stable ordering\n",
        "    unique_tokens = list(dict.fromkeys(t[2] for t in triples))\n",
        "\n",
        "    # Resolve all unique tokens concurrently\n",
        "    resolved_lists = await asyncio.gather(\n",
        "        *[resolve_title_token_ids(token) for token in unique_tokens],\n",
        "    )\n",
        "    token_to_ids = {\n",
        "        token: ids\n",
        "        for token, ids in zip(unique_tokens, resolved_lists)\n",
        "        if ids\n",
        "    }\n",
        "\n",
        "    # Build per-search token maps from triples\n",
        "    title_token_maps: list[dict[int, list[int]]] = []\n",
        "    for search_idx, title_tokens in enumerate(include_title_searches):\n",
        "        token_map: dict[int, list[int]] = {}\n",
        "        for token_idx, token in enumerate(title_tokens):\n",
        "            ids = token_to_ids.get(token)\n",
        "            if ids:\n",
        "                token_map[token_idx] = ids\n",
        "        title_token_maps.append(token_map)\n",
        "\n",
        "    return title_token_maps\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  2. Eligible Set Construction — Metadata Hard Filters (Lexical Guide §8)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "def _build_eligible_cte(filters: MetadataFilters) -> tuple[str, list]:\n",
        "    \"\"\"\n",
        "    Build the SQL fragment and parameter list for a MATERIALIZED eligible-set\n",
        "    CTE against public.movie_card.\n",
        "\n",
        "    Returns:\n",
        "        (cte_sql, params) where cte_sql is the full\n",
        "        ``eligible AS MATERIALIZED (...)`` block ready to prepend into a\n",
        "        WITH chain, and params is the ordered list of bind values.\n",
        "    \"\"\"\n",
        "    conditions: list[str] = []\n",
        "    params: list = []\n",
        "\n",
        "    if filters.min_release_ts is not None:\n",
        "        conditions.append(\"release_ts >= %s\")\n",
        "        params.append(filters.min_release_ts)\n",
        "\n",
        "    if filters.max_release_ts is not None:\n",
        "        conditions.append(\"release_ts <= %s\")\n",
        "        params.append(filters.max_release_ts)\n",
        "\n",
        "    if filters.min_runtime is not None:\n",
        "        conditions.append(\"runtime_minutes >= %s\")\n",
        "        params.append(filters.min_runtime)\n",
        "\n",
        "    if filters.max_runtime is not None:\n",
        "        conditions.append(\"runtime_minutes <= %s\")\n",
        "        params.append(filters.max_runtime)\n",
        "\n",
        "    if filters.max_maturity_rank is not None:\n",
        "        conditions.append(\"maturity_rank <= %s\")\n",
        "        params.append(filters.max_maturity_rank)\n",
        "\n",
        "    if filters.genre_ids is not None:\n",
        "        conditions.append(\"genre_ids && %s::int[]\")\n",
        "        params.append(filters.genre_ids)\n",
        "\n",
        "    if filters.watch_offer_keys is not None:\n",
        "        conditions.append(\"watch_offer_keys && %s::int[]\")\n",
        "        params.append(filters.watch_offer_keys)\n",
        "\n",
        "    where_clause = \" AND \".join(conditions) if conditions else \"TRUE\"\n",
        "\n",
        "    cte_sql = (\n",
        "        f\"eligible AS MATERIALIZED (\\n\"\n",
        "        f\"            SELECT movie_id\\n\"\n",
        "        f\"            FROM public.movie_card\\n\"\n",
        "        f\"            WHERE {where_clause}\\n\"\n",
        "        f\"        )\"\n",
        "    )\n",
        "    return cte_sql, params\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  3. Posting-Table Searches — Phrase Buckets (Lexical Guide §9.2)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# Allowed posting tables — used as a safeguard against SQL injection since\n",
        "# the table name is interpolated into the query string.\n",
        "_VALID_POSTING_TABLES: frozenset[str] = frozenset({\n",
        "    \"lex.inv_person_postings\",\n",
        "    \"lex.inv_studio_postings\",\n",
        "})\n",
        "\n",
        "async def _search_phrase_postings(\n",
        "    table: str,\n",
        "    term_ids: list[int],\n",
        "    filters: Optional[MetadataFilters] = None,\n",
        ") -> dict[int, int]:\n",
        "    \"\"\"\n",
        "    Count distinct matched term_ids per movie from a phrase posting table.\n",
        "\n",
        "    Builds a MATERIALIZED eligible CTE when metadata filters are active,\n",
        "    otherwise queries the posting table directly.\n",
        "\n",
        "    Args:\n",
        "        table:    Fully-qualified posting table name (must be in\n",
        "                  _VALID_POSTING_TABLES).\n",
        "        term_ids: Resolved string_ids for INCLUDE phrases in this bucket.\n",
        "        filters:  Optional metadata hard-filters.\n",
        "\n",
        "    Returns:\n",
        "        Dict of {movie_id: matched_count}.\n",
        "    \"\"\"\n",
        "    if not term_ids:\n",
        "        return {}\n",
        "\n",
        "    if table not in _VALID_POSTING_TABLES:\n",
        "        raise ValueError(f\"Invalid posting table: {table!r}\")\n",
        "\n",
        "    use_eligible = filters is not None and filters.is_active\n",
        "\n",
        "    cte_parts: list[str] = []\n",
        "    params: list = []\n",
        "\n",
        "    if use_eligible:\n",
        "        eligible_cte, eligible_params = _build_eligible_cte(filters)\n",
        "        cte_parts.append(eligible_cte)\n",
        "        params.extend(eligible_params)\n",
        "\n",
        "    params.append(term_ids)\n",
        "\n",
        "    eligibility_join = (\n",
        "        \"\\n            JOIN eligible e ON e.movie_id = p.movie_id\"\n",
        "        if use_eligible\n",
        "        else \"\"\n",
        "    )\n",
        "\n",
        "    with_clause = f\"WITH {cte_parts[0]}\\n        \" if cte_parts else \"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "        {with_clause}SELECT p.movie_id, COUNT(DISTINCT p.term_id)::int AS matched\n",
        "        FROM {table} p{eligibility_join}\n",
        "        WHERE p.term_id = ANY(%s::bigint[])\n",
        "        GROUP BY p.movie_id\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        async with pool.connection() as conn:\n",
        "            async with conn.cursor() as cur:\n",
        "                await cur.execute(query, params)\n",
        "                rows = await cur.fetchall()\n",
        "    except Exception:\n",
        "        # TODO - Log here\n",
        "        raise\n",
        "\n",
        "    return {row[0]: row[1] for row in rows}\n",
        "\n",
        "\n",
        "async def search_people_postings(\n",
        "    people_term_ids: list[int],\n",
        "    filters: Optional[MetadataFilters] = None,\n",
        ") -> dict[int, int]:\n",
        "    \"\"\"\n",
        "    Count distinct matched people per movie.\n",
        "\n",
        "    Args:\n",
        "        people_term_ids: Resolved string_ids for INCLUDE people phrases.\n",
        "        filters:         Optional metadata hard-filters.\n",
        "\n",
        "    Returns:\n",
        "        Dict of {movie_id: matched_people_count}.\n",
        "    \"\"\"\n",
        "    return await _search_phrase_postings(\n",
        "        \"lex.inv_person_postings\", people_term_ids, filters,\n",
        "    )\n",
        "\n",
        "\n",
        "async def search_studio_postings(\n",
        "    studio_term_ids: list[int],\n",
        "    filters: Optional[MetadataFilters] = None,\n",
        ") -> dict[int, int]:\n",
        "    \"\"\"\n",
        "    Count distinct matched studios per movie.\n",
        "\n",
        "    Args:\n",
        "        studio_term_ids: Resolved string_ids for INCLUDE studio phrases.\n",
        "        filters:         Optional metadata hard-filters.\n",
        "\n",
        "    Returns:\n",
        "        Dict of {movie_id: matched_studio_count}.\n",
        "    \"\"\"\n",
        "    return await _search_phrase_postings(\n",
        "        \"lex.inv_studio_postings\", studio_term_ids, filters,\n",
        "    )\n",
        "\n",
        "\n",
        "async def search_character_postings(\n",
        "    character_phrases: list[str],\n",
        "    filters: Optional[MetadataFilters] = None,\n",
        ") -> dict[int, int]:\n",
        "    \"\"\"\n",
        "    Count distinct matched character query phrases per movie.\n",
        "\n",
        "    Calls resolve_character_term_ids to get substring-expanded term_ids,\n",
        "    then runs the standard posting join pattern: unnest (query_idx, term_id)\n",
        "    pairs, join inv_character_postings, COUNT(DISTINCT query_idx) per movie.\n",
        "\n",
        "    Args:\n",
        "        character_phrases: Normalized character phrase strings.\n",
        "        filters:           Optional metadata hard-filters.\n",
        "\n",
        "    Returns:\n",
        "        Dict of {movie_id: matched_character_count}.\n",
        "    \"\"\"\n",
        "    if not character_phrases:\n",
        "        return {}\n",
        "\n",
        "    # Phase 1: resolve phrases → term_ids\n",
        "    phrase_term_map = await resolve_character_term_ids(character_phrases)\n",
        "\n",
        "    if not phrase_term_map:\n",
        "        return {}\n",
        "\n",
        "    # Flatten into parallel arrays for unnest\n",
        "    query_idxs: list[int] = []\n",
        "    term_ids: list[int] = []\n",
        "    for q_idx, tids in phrase_term_map.items():\n",
        "        for tid in tids:\n",
        "            query_idxs.append(q_idx)\n",
        "            term_ids.append(tid)\n",
        "\n",
        "    if not term_ids:\n",
        "        return {}\n",
        "\n",
        "    # Phase 2: posting join\n",
        "    use_eligible = filters is not None and filters.is_active\n",
        "\n",
        "    cte_parts: list[str] = []\n",
        "    params: list = []\n",
        "\n",
        "    if use_eligible:\n",
        "        eligible_cte, eligible_params = _build_eligible_cte(filters)\n",
        "        cte_parts.append(eligible_cte)\n",
        "        params.extend(eligible_params)\n",
        "\n",
        "    params.extend([query_idxs, term_ids])\n",
        "    cte_parts.append(\n",
        "        \"\"\"q_chars AS (\n",
        "            SELECT unnest(%s::int[]) AS query_idx,\n",
        "                   unnest(%s::bigint[]) AS term_id\n",
        "        )\"\"\"\n",
        "    )\n",
        "\n",
        "    eligibility_join = (\n",
        "        \"\\n            JOIN eligible e ON e.movie_id = p.movie_id\"\n",
        "        if use_eligible\n",
        "        else \"\"\n",
        "    )\n",
        "    cte_parts.append(\n",
        "        f\"\"\"character_matches AS (\n",
        "            SELECT\n",
        "                p.movie_id,\n",
        "                COUNT(DISTINCT qc.query_idx)::int AS matched\n",
        "            FROM q_chars qc\n",
        "            JOIN lex.inv_character_postings p\n",
        "              ON p.term_id = qc.term_id{eligibility_join}\n",
        "            GROUP BY p.movie_id\n",
        "        )\"\"\"\n",
        "    )\n",
        "\n",
        "    with_clause = \"WITH \" + \",\\n        \".join(cte_parts)\n",
        "    query = f\"\"\"\n",
        "        {with_clause}\n",
        "        SELECT movie_id, matched\n",
        "        FROM character_matches\n",
        "    \"\"\"\n",
        "\n",
        "    async with pool.connection() as conn:\n",
        "        async with conn.cursor() as cur:\n",
        "            await cur.execute(query, params)\n",
        "            rows = await cur.fetchall()\n",
        "\n",
        "    return {row[0]: row[1] for row in rows}\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  4. Title Postings Search + F-Score (Lexical Guide §9.3)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "async def search_title_postings(\n",
        "    token_term_id_map: dict[int, list[int]],\n",
        "    filters: Optional[MetadataFilters] = None,\n",
        ") -> dict[int, float]:\n",
        "    \"\"\"\n",
        "    Compute title F-scores for one title search by joining token postings.\n",
        "\n",
        "    Builds a MATERIALIZED eligible CTE when metadata filters are active\n",
        "    (avoiding large array transfer), counts matched token positions (m)\n",
        "    per movie, then applies the coverage-weighted F-score:\n",
        "\n",
        "        title_score = (1+β²)·(coverage·specificity) / (β²·specificity + coverage)\n",
        "\n",
        "    where coverage = m/k, specificity = m/L, β = TITLE_SCORE_BETA.\n",
        "\n",
        "    Results are capped at TITLE_MAX_CANDIDATES (sorted by score desc) in\n",
        "    the SQL query itself for wire-transfer efficiency.\n",
        "\n",
        "    Args:\n",
        "        token_term_id_map: Mapping of token_idx → list[term_id].\n",
        "            Each key is a positional index for a query token; values are\n",
        "            all candidate string_ids that fuzzy-matched that position.\n",
        "        filters: Optional metadata hard-filters.  When active, an\n",
        "            eligible-set CTE is built inline rather than passing IDs.\n",
        "\n",
        "    Returns:\n",
        "        Dict of {movie_id: title_score} for qualifying movies, capped\n",
        "        at TITLE_MAX_CANDIDATES entries.\n",
        "    \"\"\"\n",
        "    if not token_term_id_map:\n",
        "        return {}\n",
        "\n",
        "    # ── Flatten the map into parallel arrays for unnest-based CTE ─────────\n",
        "    token_idxs: list[int] = []\n",
        "    term_ids: list[int] = []\n",
        "    for token_idx, tids in token_term_id_map.items():\n",
        "        for tid in tids:\n",
        "            token_idxs.append(token_idx)\n",
        "            term_ids.append(tid)\n",
        "\n",
        "    if not term_ids:\n",
        "        return {}\n",
        "\n",
        "    k = len(token_term_id_map)\n",
        "\n",
        "    # Precompute F-score coefficients\n",
        "    beta_sq = TITLE_SCORE_BETA ** 2\n",
        "    f_coeff = 1.0 + beta_sq\n",
        "\n",
        "    # ── Dynamically build query pieces ────────────────────────────────────\n",
        "    use_eligible = filters is not None and filters.is_active\n",
        "\n",
        "    cte_prefix_parts: list[str] = []\n",
        "    params: list = []\n",
        "\n",
        "    # 1) Eligible CTE (only when filters are active)\n",
        "    if use_eligible:\n",
        "        eligible_cte, eligible_params = _build_eligible_cte(filters)\n",
        "        cte_prefix_parts.append(eligible_cte)\n",
        "        params.extend(eligible_params)\n",
        "\n",
        "    # 2) q_tokens CTE (always present)\n",
        "    params.extend([token_idxs, term_ids])\n",
        "    cte_prefix_parts.append(\n",
        "        \"\"\"q_tokens AS (\n",
        "            SELECT unnest(%s::int[]) AS token_idx,\n",
        "                   unnest(%s::bigint[]) AS term_id\n",
        "        )\"\"\"\n",
        "    )\n",
        "\n",
        "    # 3) token_matches + title_matches CTEs — one match per query token (no fuzzy expansion bias)\n",
        "    # First get distinct (movie_id, token_idx) so each of the n query tokens contributes\n",
        "    # at most 1 regardless of how many fuzzy term_ids it expanded to.\n",
        "    eligibility_join = (\n",
        "        \"\\n                JOIN eligible e ON e.movie_id = p.movie_id\"\n",
        "        if use_eligible\n",
        "        else \"\"\n",
        "    )\n",
        "    cte_prefix_parts.append(\n",
        "        f\"\"\"token_matches AS (\n",
        "            SELECT DISTINCT p.movie_id, qt.token_idx\n",
        "            FROM q_tokens qt\n",
        "            JOIN lex.inv_title_token_postings p\n",
        "              ON p.term_id = qt.term_id{eligibility_join}\n",
        "        ),\n",
        "        title_matches AS (\n",
        "            SELECT movie_id, COUNT(*)::int AS m\n",
        "            FROM token_matches\n",
        "            GROUP BY movie_id\n",
        "        )\"\"\"\n",
        "    )\n",
        "\n",
        "    # 4) title_scored CTE — applies F-score formula\n",
        "    params.extend([f_coeff, k, beta_sq, k])\n",
        "    cte_prefix_parts.append(\n",
        "        \"\"\"title_scored AS (\n",
        "            SELECT\n",
        "                tm.movie_id,\n",
        "                (%s::double precision\n",
        "                    * ((tm.m::double precision / %s)\n",
        "                       * (tm.m::double precision / mc.title_token_count)))\n",
        "                / (%s::double precision\n",
        "                    * (tm.m::double precision / mc.title_token_count)\n",
        "                    + (tm.m::double precision / %s))\n",
        "                AS title_score\n",
        "            FROM title_matches tm\n",
        "            JOIN public.movie_card mc ON mc.movie_id = tm.movie_id\n",
        "            WHERE mc.title_token_count > 0\n",
        "              AND %s > 0\n",
        "        )\"\"\"\n",
        "    )\n",
        "    params.append(k)  # k > 0 guard bound into the WHERE clause\n",
        "\n",
        "    # 5) Final SELECT with threshold filter + safety cap\n",
        "    params.extend([TITLE_SCORE_THRESHOLD, TITLE_MAX_CANDIDATES])\n",
        "\n",
        "    with_clause = \"WITH \" + \",\\n        \".join(cte_prefix_parts)\n",
        "    query = f\"\"\"\n",
        "        {with_clause}\n",
        "        SELECT movie_id, title_score\n",
        "        FROM title_scored\n",
        "        WHERE title_score >= %s\n",
        "        ORDER BY title_score DESC\n",
        "        LIMIT %s\n",
        "    \"\"\"\n",
        "\n",
        "    # ── Execute ───────────────────────────────────────────────────────────\n",
        "    try:\n",
        "        async with pool.connection() as conn:\n",
        "            async with conn.cursor() as cur:\n",
        "                await cur.execute(query, params)\n",
        "                rows = await cur.fetchall()\n",
        "    except Exception:\n",
        "        # TODO - Log here\n",
        "        raise\n",
        "\n",
        "    return {row[0]: float(row[1]) for row in rows}\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  5. Bulk Movie Card Fetch — Metadata Enrichment (DB Guide Part 3 Step 3)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "\n",
        "async def fetch_movie_cards(movie_ids: list[int]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Bulk fetch canonical movie metadata from public.movie_card.\n",
        "\n",
        "    Single query for all candidates — never per-candidate.  Results feed both\n",
        "    the reranker (metadata preference scoring) and the final API response\n",
        "    payload (card rendering).\n",
        "\n",
        "    Args:\n",
        "        movie_ids: List of movie IDs to fetch metadata for.\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with keys: movie_id, title, poster_url,\n",
        "        release_ts, runtime_minutes, maturity_rank, genre_ids,\n",
        "        watch_offer_keys, audio_language_ids, reception_score.\n",
        "    \"\"\"\n",
        "    if not movie_ids:\n",
        "        return []\n",
        "\n",
        "    query = \"\"\"\n",
        "        SELECT movie_id, title, poster_url, release_ts, runtime_minutes,\n",
        "               maturity_rank, genre_ids, watch_offer_keys, audio_language_ids,\n",
        "               reception_score\n",
        "        FROM public.movie_card\n",
        "        WHERE movie_id = ANY(%s::bigint[])\n",
        "    \"\"\"\n",
        "    columns = [\n",
        "        \"movie_id\", \"title\", \"poster_url\", \"release_ts\",\n",
        "        \"runtime_minutes\", \"maturity_rank\", \"genre_ids\", \"watch_offer_keys\",\n",
        "        \"audio_language_ids\", \"reception_score\",\n",
        "    ]\n",
        "\n",
        "    async with pool.connection() as conn:\n",
        "        async with conn.cursor() as cur:\n",
        "            await cur.execute(query, (movie_ids,))\n",
        "            rows = await cur.fetchall()\n",
        "\n",
        "    return [dict(zip(columns, row)) for row in rows]\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  6. EXCLUDE Anti-Join Helper (Lexical Guide §9.7)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "\n",
        "async def _fetch_excluded_movie_ids(\n",
        "    posting_table: str,\n",
        "    exclude_term_ids: list[int],\n",
        "    candidate_movie_ids: set[int],\n",
        ") -> set[int]:\n",
        "    \"\"\"\n",
        "    Find candidate movies that match any excluded term in a posting table.\n",
        "\n",
        "    Returns the subset of candidate_movie_ids that have at least one posting\n",
        "    for an excluded term, so the caller can remove them from results.\n",
        "\n",
        "    Args:\n",
        "        posting_table:      Fully-qualified table name\n",
        "                            (e.g. \"lex.inv_person_postings\").\n",
        "        exclude_term_ids:   Term IDs to check for exclusion.\n",
        "        candidate_movie_ids: Current candidate set to filter against.\n",
        "\n",
        "    Returns:\n",
        "        Set of movie_ids that should be excluded.\n",
        "    \"\"\"\n",
        "    if not exclude_term_ids or not candidate_movie_ids:\n",
        "        return set()\n",
        "\n",
        "    # Whitelist to prevent SQL injection via table name interpolation\n",
        "    allowed_tables = {\n",
        "        \"lex.inv_person_postings\",\n",
        "        \"lex.inv_character_postings\",\n",
        "        \"lex.inv_studio_postings\",\n",
        "        \"lex.inv_title_token_postings\",\n",
        "    }\n",
        "    if posting_table not in allowed_tables:\n",
        "        raise ValueError(f\"Unknown posting table: {posting_table}\")\n",
        "\n",
        "    query = f\"\"\"\n",
        "        SELECT DISTINCT p.movie_id\n",
        "        FROM {posting_table} p\n",
        "        WHERE p.term_id = ANY(%s::bigint[])\n",
        "          AND p.movie_id = ANY(%s::bigint[])\n",
        "    \"\"\"\n",
        "    async with pool.connection() as conn:\n",
        "        async with conn.cursor() as cur:\n",
        "            await cur.execute(query, (exclude_term_ids, list(candidate_movie_ids)))\n",
        "            rows = await cur.fetchall()\n",
        "\n",
        "    return {row[0] for row in rows}\n",
        "\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  7. Full Lexical Search Orchestration (Lexical Guide §10)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "\n",
        "async def lexical_search(\n",
        "    include_people: Optional[list[str]] = None,\n",
        "    include_characters: Optional[list[str]] = None,\n",
        "    include_studios: Optional[list[str]] = None,\n",
        "    include_title_searches: Optional[list[list[str]]] = None,\n",
        "    exclude_people: Optional[list[str]] = None,\n",
        "    exclude_characters: Optional[list[str]] = None,\n",
        "    exclude_studios: Optional[list[str]] = None,\n",
        "    min_release_ts: Optional[int] = None,\n",
        "    max_release_ts: Optional[int] = None,\n",
        "    min_runtime: Optional[int] = None,\n",
        "    max_runtime: Optional[int] = None,\n",
        "    max_maturity_rank: Optional[int] = None,\n",
        "    genre_ids: Optional[list[int]] = None,\n",
        "    watch_offer_keys: Optional[list[int]] = None,\n",
        ") -> list[LexicalCandidate]:\n",
        "    \"\"\"\n",
        "    Full lexical search combining all buckets with OR semantics.\n",
        "\n",
        "    Orchestrates the complete flow described in Lexical Guide §10:\n",
        "      1. Build eligible set (metadata hard filters)\n",
        "      2. Resolve phrase term IDs (exact dictionary lookup)\n",
        "      3. Resolve title token IDs (fuzzy per token)\n",
        "      4. Run all posting searches concurrently\n",
        "      5. Aggregate title scores across multiple title searches\n",
        "      6. Apply title candidate safety cap\n",
        "      7. Union candidates from all buckets (OR semantics)\n",
        "      8. Apply EXCLUDE anti-joins\n",
        "      9. Compute raw_lexical_score per candidate\n",
        "\n",
        "    Normalise server-side: lexical_score = raw / max_possible, where\n",
        "    max_possible = #people + #characters + #studios + #title_searches.\n",
        "\n",
        "    All phrase / token inputs should be pre-normalized strings.\n",
        "    Title searches are lists of token lists (OR-of-phrases, e.g.\n",
        "    [[\"kung\", \"fu\"], [\"big\", \"shot\"]]).\n",
        "\n",
        "    Args:\n",
        "        include_people:         Normalized people phrase strings.\n",
        "        include_characters:     Normalized character phrase strings.\n",
        "        include_studios:        Normalized studio phrase strings.\n",
        "        include_title_searches: List of token lists, each one title search.\n",
        "        exclude_people:         Normalized people phrases to hard-exclude.\n",
        "        exclude_characters:     Normalized character phrases to hard-exclude.\n",
        "        exclude_studios:        Normalized studio phrases to hard-exclude.\n",
        "        min_release_ts:         Minimum release timestamp filter.\n",
        "        max_release_ts:         Maximum release timestamp filter.\n",
        "        min_runtime:            Minimum runtime filter (minutes).\n",
        "        max_runtime:            Maximum runtime filter (minutes).\n",
        "        max_maturity_rank:      Maximum maturity rank filter (≤).\n",
        "        genre_ids:              Genre overlap filter IDs.\n",
        "        watch_offer_keys:       Watch-offer overlap filter keys.\n",
        "\n",
        "    Returns:\n",
        "        List of LexicalCandidate objects sorted by raw_lexical_score desc.\n",
        "    \"\"\"\n",
        "    include_people = include_people or []\n",
        "    include_characters = include_characters or []\n",
        "    include_studios = include_studios or []\n",
        "    include_title_searches = include_title_searches or []\n",
        "    exclude_people = exclude_people or []\n",
        "    exclude_characters = exclude_characters or []\n",
        "    exclude_studios = exclude_studios or []\n",
        "\n",
        "    # ── Steps 1–3: Run eligible set, phrase resolution, and title token\n",
        "    #     resolution concurrently (all are independent) ─────────────────────\n",
        "    all_phrases = list(set(\n",
        "        include_people + include_characters + include_studios\n",
        "        + exclude_people + exclude_characters + exclude_studios\n",
        "    ))\n",
        "    eligible_set, phrase_id_map, title_token_maps = await asyncio.gather(\n",
        "        build_eligible_set(\n",
        "            min_release_ts=min_release_ts,\n",
        "            max_release_ts=max_release_ts,\n",
        "            min_runtime=min_runtime,\n",
        "            max_runtime=max_runtime,\n",
        "            max_maturity_rank=max_maturity_rank,\n",
        "            genre_ids=genre_ids,\n",
        "            watch_offer_keys=watch_offer_keys,\n",
        "        ),\n",
        "        resolve_phrase_term_ids(all_phrases),\n",
        "        _resolve_all_title_tokens(include_title_searches),\n",
        "    )\n",
        "    eligible_ids = list(eligible_set.keys()) if eligible_set is not None else None\n",
        "\n",
        "    people_term_ids = [phrase_id_map[p] for p in include_people if p in phrase_id_map]\n",
        "    character_term_ids = [phrase_id_map[c] for c in include_characters if c in phrase_id_map]\n",
        "    studio_term_ids = [phrase_id_map[s] for s in include_studios if s in phrase_id_map]\n",
        "\n",
        "    exclude_people_ids = [phrase_id_map[p] for p in exclude_people if p in phrase_id_map]\n",
        "    exclude_character_ids = [phrase_id_map[c] for c in exclude_characters if c in phrase_id_map]\n",
        "    exclude_studio_ids = [phrase_id_map[s] for s in exclude_studios if s in phrase_id_map]\n",
        "\n",
        "    # ── Step 4: Run all posting searches concurrently ─────────────────────\n",
        "    people_future = search_people_postings(people_term_ids, eligible_ids)\n",
        "    character_future = search_character_postings(character_term_ids, eligible_ids)\n",
        "    studio_future = search_studio_postings(studio_term_ids, eligible_ids)\n",
        "    title_futures = [\n",
        "        search_title_postings(token_map, eligible_ids)\n",
        "        for token_map in title_token_maps\n",
        "    ]\n",
        "\n",
        "    all_results = await asyncio.gather(\n",
        "        people_future, character_future, studio_future, *title_futures,\n",
        "    )\n",
        "\n",
        "    people_scores: dict[int, int] = all_results[0]\n",
        "    character_scores: dict[int, int] = all_results[1]\n",
        "    studio_scores: dict[int, int] = all_results[2]\n",
        "\n",
        "    # ── Step 5: Aggregate title scores (sum across title searches) ────────\n",
        "    title_score_results: list[dict[int, float]] = list(all_results[3:])\n",
        "    title_score_sums: dict[int, float] = {}\n",
        "    for title_result in title_score_results:\n",
        "        for movie_id, score in title_result.items():\n",
        "            title_score_sums[movie_id] = title_score_sums.get(movie_id, 0.0) + score\n",
        "\n",
        "    # ── Step 6: Title candidate safety cap ────────────────────────────────\n",
        "    if len(title_score_sums) > TITLE_MAX_CANDIDATES:\n",
        "        sorted_titles = sorted(title_score_sums.items(), key=lambda x: x[1], reverse=True)\n",
        "        title_score_sums = dict(sorted_titles[:TITLE_MAX_CANDIDATES])\n",
        "\n",
        "    # ── Step 7: Union all candidates (OR semantics) ───────────────────────\n",
        "    all_movie_ids: set[int] = set()\n",
        "    all_movie_ids.update(people_scores.keys())\n",
        "    all_movie_ids.update(character_scores.keys())\n",
        "    all_movie_ids.update(studio_scores.keys())\n",
        "    all_movie_ids.update(title_score_sums.keys())\n",
        "\n",
        "    # ── Step 8: EXCLUDE anti-joins (run concurrently) ────────────────────────\n",
        "    exclude_futures = []\n",
        "    if exclude_people_ids:\n",
        "        exclude_futures.append(\n",
        "            _fetch_excluded_movie_ids(\n",
        "                \"lex.inv_person_postings\", exclude_people_ids, all_movie_ids,\n",
        "            )\n",
        "        )\n",
        "    if exclude_character_ids:\n",
        "        exclude_futures.append(\n",
        "            _fetch_excluded_movie_ids(\n",
        "                \"lex.inv_character_postings\", exclude_character_ids, all_movie_ids,\n",
        "            )\n",
        "        )\n",
        "    if exclude_studio_ids:\n",
        "        exclude_futures.append(\n",
        "            _fetch_excluded_movie_ids(\n",
        "                \"lex.inv_studio_postings\", exclude_studio_ids, all_movie_ids,\n",
        "            )\n",
        "        )\n",
        "    exclude_results = await asyncio.gather(*exclude_futures) if exclude_futures else []\n",
        "    exclude_movie_ids: set[int] = set()\n",
        "    for excluded in exclude_results:\n",
        "        exclude_movie_ids.update(excluded)\n",
        "\n",
        "    # ── Step 9: Assemble candidates with per-bucket scores ────────────────\n",
        "    candidates: list[LexicalCandidate] = []\n",
        "    for movie_id in all_movie_ids:\n",
        "        if movie_id in exclude_movie_ids:\n",
        "            continue\n",
        "\n",
        "        matched_people = people_scores.get(movie_id, 0)\n",
        "        matched_characters = character_scores.get(movie_id, 0)\n",
        "        matched_studios = studio_scores.get(movie_id, 0)\n",
        "        title_sum = title_score_sums.get(movie_id, 0.0)\n",
        "        raw = matched_people + matched_characters + matched_studios + title_sum\n",
        "\n",
        "        candidates.append(LexicalCandidate(\n",
        "            movie_id=movie_id,\n",
        "            matched_people_count=matched_people,\n",
        "            matched_character_count=matched_characters,\n",
        "            matched_studio_count=matched_studios,\n",
        "            title_score_sum=title_sum,\n",
        "            raw_lexical_score=raw,\n",
        "        ))\n",
        "\n",
        "    candidates.sort(key=lambda c: c.raw_lexical_score, reverse=True)\n",
        "    return candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c167edae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'marvel studios': 6262}\n",
            "captain america: the first avenger\n",
            "avengers: endgame\n"
          ]
        }
      ],
      "source": [
        "matches = await resolve_phrase_term_ids([\"marvel studios\"])\n",
        "\n",
        "print(matches)\n",
        "\n",
        "matches = await search_studio_postings(list(matches.values()), None)\n",
        "\n",
        "movie_cards = await fetch_movie_cards(list(matches.keys()))\n",
        "\n",
        "for card in movie_cards:\n",
        "    print(f\"{card['title']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "11b64c33",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1584: 1, 299534: 1, 569094: 1}\n",
            "school of rock - 1\n",
            "spider-man: across the spider-verse - 1\n",
            "avengers: endgame - 1\n"
          ]
        }
      ],
      "source": [
        "filters = MetadataFilters(min_release_ts=1000000000)\n",
        "\n",
        "matches = await search_character_postings([\"spider\"], filters)\n",
        "\n",
        "print(matches)\n",
        "\n",
        "movie_cards = await fetch_movie_cards(list(matches.keys()))\n",
        "\n",
        "for card in movie_cards:\n",
        "    print(f\"{card['title']} - {matches[card['movie_id']]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0c1e25",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Methods to test\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  4. Title Postings Search + F-Score (Lexical Guide §9.3)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# async def search_title_postings()\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  6. EXCLUDE Anti-Join Helper (Lexical Guide §9.7)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# async def _fetch_excluded_movie_ids()\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "#  7. Full Lexical Search Orchestration (Lexical Guide §10)\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# async def lexical_search()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
